{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d0363c-2391-4879-a566-b4b20d01db9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/gsk6me/huggingface_cache'\n",
    "\n",
    "gpt2 = transformers.GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(\"cuda\")\n",
    "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a352ba-92ad-496f-b07e-64926b698096",
   "metadata": {},
   "source": [
    "# Step 1: Remove `elementwise_affine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d1ce39-d71c-4061-b906-b8d0a4738276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a4f49-d3e8-4045-8a50-a34edfe816bd",
   "metadata": {},
   "source": [
    "To get rid of elementwise_affine, we must first understand what it does:\n",
    "$$\n",
    "\\operatorname{LayerNorm}(x) = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\operatorname{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "$$\n",
    "Where $\\gamma, \\beta \\in \\mathbb R^{d_{model}}$. Thus we can try to fix this moving from the last layer to the first. Starting from the unembedding, we can just subtract $\\beta$ from each of the unembedding parameters, and then divide the unembedding weights by $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8603207-fe46-4532-9bff-481394f32726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.h[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cef0db-e664-4921-b070-b3621b3565f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can we get rid of elementwise_affine?\n",
    "gpt2.transformer.h[0].ln_1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b98ddb6-bb24-40ab-8316-79b7c8760c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5770e7-a197-4507-84ce-4cc9f80b31c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.h[-1].mlp.c_fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffba48d9-c61e-45d0-a36d-54ae0498facb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb9f1dc-2e97-4e12-8b6f-dd15cafe6ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "residual = hidden_states\n",
    "hidden_states = self.ln_2(hidden_states)\n",
    "feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "# residual connection\n",
    "hidden_states = residual + feed_forward_hidden_states\n",
    "\"\"\"\n",
    "\n",
    "ln_non_affine = torch.nn.LayerNorm(normalized_shape=(768,)).to('cuda')\n",
    "\n",
    "inp = torch.arange(768, device='cuda').float()\n",
    "\n",
    "ln = gpt2.transformer.h[-1].ln_2\n",
    "with torch.no_grad():\n",
    "    r = (ln(inp) - ln.bias)/ln.weight\n",
    "torch.allclose(r, ln_non_affine(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7b1c69-dba4-4685-b55f-037088ebc968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lnw_orig = ln.weight.clone()\n",
    "lnb_orig = ln.bias.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064bbc75-7dfe-4e8e-b0f4-341649e7b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_orig = gpt2.transformer.h[-1].mlp.c_fc.weight.clone()\n",
    "bias_orig = gpt2.transformer.h[-1].mlp.c_fc.bias.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec4add5-5680-488d-ada3-4b5913644aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.h[-1].mlp.c_fc(ln.bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cbe62a-c02c-4224-887b-f54692729a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0647,  0.3438,  0.5384,  ..., -0.1176, -0.1933, -0.2486],\n",
      "       device='cuda:0', grad_fn=<MvBackward0>) tensor([-0.0379,  0.4127,  0.6849,  ..., -0.0568, -0.1684, -0.2452],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
       "           -37.9090],\n",
       "         [ -69.5277,  -65.2476,  -74.2180,  ...,  -80.8885,  -76.5055,\n",
       "           -70.8534],\n",
       "         [ -93.6960,  -93.7839,  -94.6315,  ..., -104.1838, -102.1104,\n",
       "           -89.3226]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = gpt2.transformer.h[-1].ln_2\n",
    "weight_rescaled = weight_orig * lnw_orig.unsqueeze(-1)\n",
    "fc = gpt2.transformer.h[-1].mlp.c_fc\n",
    "gpt2.transformer.h[-1].mlp.c_fc.weight.data = weight_rescaled\n",
    "gpt2.transformer.h[-1].mlp.c_fc.bias.data = bias_orig + weight_orig.T @ lnb_orig\n",
    "print(weight_orig.T@lnb_orig, fc(lnb_orig) - bias_orig)\n",
    "gpt2.transformer.h[-1].ln_2.elementwise_affine = False\n",
    "with torch.no_grad():\n",
    "    gpt2.transformer.h[-1].ln_2.weight[:] = 1\n",
    "    gpt2.transformer.h[-1].ln_2.bias[:] = 0\n",
    "with torch.no_grad():\n",
    "    result_rescaled = gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda'))\n",
    "result_rescaled.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d2bb8d-28d7-42db-a5f1-20792f23d1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
       "           -37.9090],\n",
       "         [ -69.5277,  -65.2476,  -74.2180,  ...,  -80.8885,  -76.5055,\n",
       "           -70.8534],\n",
       "         [ -93.6960,  -93.7839,  -94.6315,  ..., -104.1838, -102.1104,\n",
       "           -89.3226]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.h[-1].mlp.c_fc.weight.data = weight_orig\n",
    "gpt2.transformer.h[-1].mlp.c_fc.bias.data = bias_orig\n",
    "gpt2.transformer.h[-1].ln_2.elementwise_affine = True\n",
    "with torch.no_grad():\n",
    "    gpt2.transformer.h[-1].ln_2.weight[:] = lnw_orig\n",
    "    gpt2.transformer.h[-1].ln_2.bias[:] = lnb_orig\n",
    "with torch.no_grad():\n",
    "    result_orig = gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda'))\n",
    "result_orig.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7345373-735a-4833-8d1c-0f68ab5023bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8147e-06,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          -3.8147e-06, -3.8147e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  7.6294e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rescaled.logits - result_orig.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84639c5b-40ad-4982-8e8f-29540b186878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_elementwise_affine_2(tblock):\n",
    "    ln2 = tblock.ln_2\n",
    "    fc1 = tblock.mlp.c_fc\n",
    "    with torch.no_grad():\n",
    "        weight_orig = fc1.weight.data.clone()\n",
    "        fc1.weight.data = fc1.weight * ln2.weight.unsqueeze(-1)\n",
    "        fc1.bias.data = fc1.bias + weight_orig.T @ ln2.bias.data\n",
    "        ln2.elementwise_affine = False\n",
    "        ln2.weight[:] = 1\n",
    "        ln2.bias[:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a23b96e-73b5-4958-b731-fcf1a01ac90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gpt2.transformer.h)):\n",
    "    remove_elementwise_affine_2(gpt2.transformer.h[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9c2adb-b7d8-48bf-8e04-be77ed488598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.8147e-06,\n",
      "           0.0000e+00, -7.6294e-06],\n",
      "         [ 0.0000e+00, -7.6294e-06, -7.6294e-06,  ..., -7.6294e-06,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  7.6294e-06,  0.0000e+00,  ...,  7.6294e-06,\n",
      "           7.6294e-06,  0.0000e+00]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda')).logits - result_orig.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea534b-ea57-4d77-81c6-f500c42a699f",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We were able to undo the notion of `elementwise_affine` for the second LayerNorm. The nice thing is that the whole residual stream doesn't seem to get layernorm'd here, which is sort of surprising actually and I'm not sure if it will need to be modified for other networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feed876-b1b1-4437-af25-bdb3c2133577",
   "metadata": {},
   "source": [
    "## Removing `ln1`'s `elementwise_affine`\n",
    "\n",
    "We will do this by modifying the `c_attn` affine transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d88ac0-4e82-4475-a01f-daee6221be57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_elementwise_affine_1(tblock):\n",
    "    ln = tblock.ln_1\n",
    "    fc = tblock.attn.c_attn\n",
    "    with torch.no_grad():\n",
    "        weight_orig = fc.weight.data.clone()\n",
    "        fc.weight.data = fc.weight * ln.weight.unsqueeze(-1)\n",
    "        fc.bias.data = fc.bias + weight_orig.T @ ln.bias.data\n",
    "        ln.elementwise_affine = False\n",
    "        ln.weight[:] = 1\n",
    "        ln.bias[:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c76cb0d2-ae2e-425f-9529-4a3100990bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tblock = gpt2.transformer.h[-1]\n",
    "Wa_orig = tblock.attn.c_attn.weight.clone()\n",
    "ba_orig = tblock.attn.c_attn.bias.clone()\n",
    "ln1w_orig = tblock.ln_1.weight.clone()\n",
    "ln1b_orig = tblock.ln_1.bias.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eece64a2-353b-4dc0-84eb-570e70e24445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = tblock.ln_1\n",
    "fc = tblock.attn.c_attn\n",
    "with torch.no_grad():\n",
    "    weight_orig = fc.weight.data.clone()\n",
    "    fc.weight.data = fc.weight * ln.weight.unsqueeze(-1)\n",
    "    fc.bias.data = fc.bias + weight_orig.T @ ln.bias.data\n",
    "    ln.elementwise_affine = False\n",
    "    ln.weight[:] = 1\n",
    "    ln.bias[:] = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    result_fixed_attn = gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda'))\n",
    "\n",
    "# gpt2.transformer.h[-1].attn.c_attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf1946c-0794-416c-aa61-916ebb0fd5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4332e-05, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result_fixed_attn.logits - result_orig.logits).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0babe48-d5b7-4e76-9d7d-4ed0dd13b8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(gpt2.transformer.h)):\n",
    "    remove_elementwise_affine_1(gpt2.transformer.h[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445656ee-48eb-443a-9a38-a2d0baf22327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8147e-05, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result_fixed_attn_all = gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda'))\n",
    "    \n",
    "(result_fixed_attn_all.logits - result_orig.logits).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5294e9db-9a2a-4eb8-b152-fe28dfa6e56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=False)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=False)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc794fd-d77b-4fd9-8059-cfb4681fa264",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "So it seems like we could remove almost all of the elementwise affine operations! There's a final `ln_f` which we will remove now. (Note that even though it says `bias=False` there *is* actually a bias because of the `ln_f`!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78d9cd7f-9554-491f-8574-11fa3bb8a842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 768]), None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head.weight.shape, gpt2.lm_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84670705-f40c-4766-b8a8-6ba523fb814b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_ln_f(model):\n",
    "    # there is originally no bias here\n",
    "    ln = model.transformer.ln_f\n",
    "    fc = model.lm_head\n",
    "    \n",
    "    '''\n",
    "    gpt2.transformer:\n",
    "    hidden_states = self.ln_f(hidden_states)\n",
    "    hidden_states = hidden_states.view(output_shape)\n",
    "    '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weight_orig = fc.weight.data.clone()\n",
    "        # needs to be done because weights are \"tied\" by HuggingFace\n",
    "        # (they point to the same thing as the original embedding matrix)\n",
    "        fc.weight = torch.nn.Parameter(fc.weight * ln.weight)\n",
    "        fc.bias = torch.nn.Parameter(weight_orig @ ln.bias.data.clone())\n",
    "\n",
    "        ln.elementwise_affine = False\n",
    "        ln.weight[:] = 1\n",
    "        ln.bias[:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e01176-72b4-4831-9f68-155e76a48017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.7654,  6.5866,  1.7553,  ...,  9.5762, 15.3242,  6.8427],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head(gpt2.transformer.ln_f(torch.arange(768, device='cuda').float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5c42f1-265d-40a5-a3b6-5b18997a0a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2 = transformers.GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(\"cuda\")\n",
    "gpt2_orig = transformers.GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(\"cuda\")\n",
    "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "lnf_b_orig = gpt2.transformer.ln_f.bias.clone()\n",
    "lnf_W_orig = gpt2.transformer.ln_f.weight.clone()\n",
    "lmh_W_orig = gpt2.lm_head.weight.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c363bfe5-c2c0-44a4-8c32-dd29afffb3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean(gpt2):\n",
    "    for i in range(12):\n",
    "        remove_elementwise_affine_1(gpt2.transformer.h[i])\n",
    "        remove_elementwise_affine_2(gpt2.transformer.h[i])\n",
    "    remove_ln_f(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93de272-b59f-4ae1-b69a-4ffa51dc9d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(gpt2):\n",
    "    return gpt2(**gpt2_tokenizer(\"hello world!\", return_tensors='pt').to('cuda'), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc3473d-55c3-421e-8741-1ef47b8ce7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3406e-05, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head.bias = None\n",
    "gpt2.load_state_dict(gpt2_orig.state_dict())\n",
    "\n",
    "clean(gpt2)\n",
    "\n",
    "result_finale = sample(gpt2)\n",
    "result_orig = sample(gpt2_orig)\n",
    "\n",
    "(result_finale.logits - result_orig.logits).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee535b4-7748-44c4-9c03-836b91268f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0617,  0.0007, -0.3006,  ..., -0.1671, -0.0038, -0.1409],\n",
       "         [-0.2276,  0.0651, -0.1504,  ..., -0.1922, -0.1107,  0.2989],\n",
       "         [ 0.0478, -0.2524, -0.1154,  ..., -0.0455, -0.0334,  0.0451]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_orig.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c2e33f-59a4-4cd4-a263-9ed093457259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
       "           -37.9090],\n",
       "         [ -69.5277,  -65.2476,  -74.2180,  ...,  -80.8885,  -76.5055,\n",
       "           -70.8534],\n",
       "         [ -93.6960,  -93.7839,  -94.6315,  ..., -104.1838, -102.1104,\n",
       "           -89.3226]]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_orig.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24eac61a-5c4d-4603-90bb-79171de03d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
       "           -37.9090],\n",
       "         [ -69.5277,  -65.2476,  -74.2180,  ...,  -80.8885,  -76.5055,\n",
       "           -70.8534],\n",
       "         [ -93.6960,  -93.7838,  -94.6315,  ..., -104.1838, -102.1103,\n",
       "           -89.3226]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head((result_orig.hidden_states[-1] - lnf_b_orig) / lnf_W_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33e20fd4-fc2c-4796-bb00-6145a35957f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.wte.weight - gpt2_orig.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac364f2-8bf5-4a02-b897-a3ba528fcdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1007, -0.3921,  0.0049,  ..., -0.2066,  0.0256,  0.0360],\n",
       "          [-0.1248,  0.0981, -0.0892,  ..., -0.2765,  0.2175,  0.0375],\n",
       "          [-0.1059, -0.1240,  0.0876,  ..., -0.1166,  0.0344,  0.0239]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[-0.1007, -0.3921,  0.0049,  ..., -0.2066,  0.0256,  0.0360],\n",
       "          [-0.1248,  0.0981, -0.0892,  ..., -0.2765,  0.2175,  0.0375],\n",
       "          [-0.1059, -0.1240,  0.0876,  ..., -0.1166,  0.0344,  0.0239]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_orig.hidden_states[0], result_finale.hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6fd8728-3b05-4c3d-ab4f-9bdd83f7f3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0450, -0.0261, -0.1236,  ..., -0.1423,  0.0198, -0.1453],\n",
       "         [-0.1637,  0.0208, -0.0440,  ..., -0.1650, -0.0776,  0.2191],\n",
       "         [ 0.0334, -0.2102, -0.0255,  ..., -0.0321, -0.0072,  0.0088]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_finale.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f6d867d-f979-4823-bcbc-e27041f07cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -37.0708,  -36.4855,  -40.3520,  ...,  -46.5168,  -45.4142,\n",
       "           -37.9090],\n",
       "         [ -69.5277,  -65.2476,  -74.2180,  ...,  -80.8885,  -76.5055,\n",
       "           -70.8534],\n",
       "         [ -93.6960,  -93.7838,  -94.6315,  ..., -104.1838, -102.1103,\n",
       "           -89.3226]]], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_finale.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24d47ee6-57fd-454d-84fa-3c4bc9b50769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ -6.3267,  -6.1134,  -8.2121,  ..., -11.1459, -10.8880,  -6.1064],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c773b5c-f6db-4af0-a9cf-eb8c9171ee6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0617,  0.0007, -0.3006,  ..., -0.1671, -0.0038, -0.1409],\n",
       "          [-0.2276,  0.0651, -0.1504,  ..., -0.1922, -0.1107,  0.2989],\n",
       "          [ 0.0478, -0.2524, -0.1154,  ..., -0.0455, -0.0334,  0.0451]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[-0.0617,  0.0007, -0.3006,  ..., -0.1671, -0.0038, -0.1409],\n",
       "          [-0.2276,  0.0651, -0.1504,  ..., -0.1922, -0.1107,  0.2989],\n",
       "          [ 0.0478, -0.2524, -0.1154,  ..., -0.0455, -0.0334,  0.0451]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result_finale.hidden_states[-1] * lnf_W_orig + lnf_b_orig), (result_orig.hidden_states[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6412e-9ad0-4f2b-a277-9fd4916b05b6",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "We have shown that we can effectively remove the `elementwise_affine` component of each LayerNorm. This will make the relationships between intermediate activations significantly more linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7c0c3-5ef2-4ede-81b4-6b9d6b1eb83d",
   "metadata": {},
   "source": [
    "## Causal relationships\n",
    "\n",
    "Let's see if we can compare correlations between MLP activations. (Using these improved matrices) So we might expect that we have bigram relationships modeled between the lm head and the word embeddings. Can also see the relationship between positional encodings and each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190942e9-da85-420e-966d-4c92549fd417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f70c4445010>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "06e6b54c-d5d2-4865-8ed9-1ef18c4c780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_corr_matrix = gpt2.lm_head.weight @ gpt2.transformer.wte.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "221dc2c2-1233-4094-8412-b3d244739d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_corr_matrix = gpt2.lm_head.weight @ gpt2.transformer.wpe.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c2d8ea5-3034-4d4f-964f-e581f2df6e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5471e+00, -5.4091e+00, -2.3274e+00,  ..., -7.3445e+00,\n",
       "         -7.6868e+00, -1.1570e-02],\n",
       "        [-1.1365e+00, -4.7076e+00, -1.4911e+00,  ..., -6.9785e+00,\n",
       "         -7.1958e+00, -2.4630e-03],\n",
       "        [-1.3125e+00, -4.2784e+00, -1.3093e+00,  ..., -7.3957e+00,\n",
       "         -7.5783e+00, -1.6481e-02],\n",
       "        ...,\n",
       "        [-4.8832e+00, -4.9813e+00, -1.7743e+00,  ..., -7.5792e+00,\n",
       "         -7.7585e+00, -1.5871e-02],\n",
       "        [-4.2381e+00, -6.3743e+00, -3.3911e+00,  ..., -8.2380e+00,\n",
       "         -8.5629e+00,  1.1326e-02],\n",
       "        [-1.4537e+00, -6.3225e+00, -2.9076e+00,  ..., -6.4535e+00,\n",
       "         -6.7289e+00, -5.0177e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bd1c52f7-63a3-4ef4-9ebd-a3adccac5bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignty\n",
      "wcsstore\n",
      "�\n",
      "estone\n",
      "ignt\n",
      "���\n",
      "eg\n",
      "arest\n",
      "ebook\n",
      "estones\n"
     ]
    }
   ],
   "source": [
    "ranked = pos_corr_matrix[:, 0].argsort(descending=True)\n",
    "for i in range(10):\n",
    "    print(gpt2_tokenizer.decode(ranked[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "862a1786-24d9-4097-be31-aed80b00e46d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32]\n",
      ">A 17.77\n",
      ">ccording 16.88\n",
      "> challeng 16.65\n",
      "> mathemat 16.59\n",
      "> destro 16.34\n",
      "> conduc 16.24\n",
      "> corrid 16.21\n",
      "> helicop 16.11\n",
      "> unden 15.93\n",
      "> misunder 15.92\n"
     ]
    }
   ],
   "source": [
    "# Result for using the transformed weights\n",
    "ids = gpt2_tokenizer(\"A\").input_ids\n",
    "print(ids)\n",
    "tok = ids[0]\n",
    "ranked = bigram_corr_matrix[:, tok].argsort(descending=True)\n",
    "for i in range(10):\n",
    "    token = gpt2_tokenizer.decode(ranked[i])\n",
    "    similarity = bigram_corr_matrix[ranked[i], tok].item()\n",
    "    print(f\">{token} {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1689d4b7-8223-4dcb-8748-9c729a349ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32]\n",
      ">A 8.25\n",
      "> A 5.61\n",
      ">An 5.50\n",
      ">a 5.36\n",
      ">E 5.31\n",
      ">The 5.29\n",
      ">B 5.24\n",
      ">I 5.24\n",
      ">C 5.18\n",
      ">D 5.09\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ids = gpt2_tokenizer(\"A\").input_ids\n",
    "    print(ids)\n",
    "    tok = ids[0]\n",
    "    bigram_corr_matrix_orig = gpt2.transformer.wte.weight @ gpt2.transformer.wte.weight.T[:, tok]\n",
    "\n",
    "    ranked = bigram_corr_matrix_orig.argsort(descending=True)\n",
    "    for i in range(10):\n",
    "        token = gpt2_tokenizer.decode(ranked[i])\n",
    "        similarity = bigram_corr_matrix_orig[ranked[i]].item()\n",
    "        print(f\">{token} {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1316d3-6fbc-4f4f-87f1-b10b6a4cd20f",
   "metadata": {},
   "source": [
    "# Step 2. See What Causes Different Outputs\n",
    "\n",
    "Now, we can represent any intermediate hidden state as a weighted sum of the MLP neuron activations that came before. And in fact, we can anticipate the activation of any given attention head solely as a function of which MLP neurons were active beforehand, and **avoid using the residual stream at all**!\n",
    "\n",
    "As a simple example, we can do the reverse of the ranking scheme I showed before, and try to find which words *induce* other words to be output.\n",
    "\n",
    "But it doesn't seem like there's a ton of insight here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a8607e3b-a7e0-4233-8ce6-244dea2c4cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282]\n",
      ">al 22.82\n",
      ">alist 19.45\n",
      ">aler 18.43\n",
      ">alis 18.16\n",
      ">als 17.80\n",
      ">alam 17.44\n",
      ">aled 17.29\n",
      ">alm 17.22\n",
      ">AL 17.22\n",
      ">aling 17.20\n"
     ]
    }
   ],
   "source": [
    "# Result for using the transformed weights\n",
    "ids = gpt2_tokenizer(\"al\").input_ids\n",
    "print(ids)\n",
    "tok = ids[0]\n",
    "ranked = bigram_corr_matrix[tok, :].argsort(descending=True)\n",
    "for i in range(10):\n",
    "    token = gpt2_tokenizer.decode(ranked[i])\n",
    "    similarity = bigram_corr_matrix[tok, ranked[i]].item()\n",
    "    print(f\">{token} {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "37205238-df09-4cbf-b581-c5829a797d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del bigram_corr_matrix\n",
    "# del pos_corr_matrix\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d1fc98cd-6807-47e9-aab2-13d1696429b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "causality = gpt2.lm_head.weight @ gpt2.transformer.h[0].mlp.c_proj.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1426ed22-6bdf-42ac-93ec-d3e26de7fb63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 3072])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causality.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa9b25-b4ac-41a0-bebd-3e1a9d52eca1",
   "metadata": {},
   "source": [
    "## Tracing what causes MLP neurons to activate\n",
    "\n",
    "To do this, we will need to search for semi-linear correlations. I think that the relationships between many of these things are quite complex. However maybe we can look at value matrices and see.\n",
    "\n",
    "I think first I'll look at what causes a given attention head to activate. We can reason about this via gradients.\n",
    "\n",
    "$$\n",
    "\\alpha_{qk} = (W_{hq}X_q)^\\top(W_{hk}X_k)\n",
    "$$\n",
    "\n",
    "We now operate under the knowledge that *all* residual states can be expressed in the form:\n",
    "$$\n",
    "\\operatorname{ResidualStream^{(\\ell)}} = \\operatorname{LayerNorm}\\left(\n",
    "    \\operatorname{WordEmbed}(\\operatorname{Word}(k)) + \\operatorname{PosEmbed}(k) + \n",
    "    \\sum_{\\ell = 0}^{L} \\sum_{j = 0}^{d_{hidden}} \\operatorname{MLPActivation}^{(\\ell)}_j \\operatorname{MLPEmbedding}^{(\\ell)}_j\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f2395-944f-49b3-bc07-6f59b1819eb1",
   "metadata": {},
   "source": [
    "Wherein Word(k) represents the token ID at index $k$, and the MLPActivation/MLPEmbedding expressions represent hidden states in the MLP.\n",
    "\n",
    "We note that the inner term is now a linear function of Word(k), PosEmbed(k), and MLPActivation$_j^{(\\ell)}$. Therefore, we can try to compile this hidden state into $S^{(\\ell)}$, abbreviating \"(s)parse state\", along with a weight matrix to project into the residual stream. Let's denote $W_{xs}$ as a weight matrix that converts from the sparse state representation into the compressed representation used in the residual stream:\n",
    "$$\n",
    "X^{(\\ell)} = \\operatorname{ResidualStream}^{(\\ell)} = \\operatorname{LayerNorm}\\left( W_{xs} S^{(\\ell)} \\right)\n",
    "$$\n",
    "We note that this is (very nearly) a linear function of $S^{(\\ell)}$, as LayerNorm simply scales and translates all vector dimensions by the standard deviation and mean. So the relative dimension activations are the same, and fairly approximately true.\n",
    "\n",
    "So let's just pretend that the LayerNorm is the identity. Now, let's try to compute the attention between key $k$ and query $q$ (taking the $\\sqrt{n}$ from the \"scaled\" in \"scaled dot-product attention\"):\n",
    "$$\n",
    "\\sqrt{n} \\alpha_{kq} = (W_k X_k)^\\top (W_q X_q) = X_k^\\top(W_k^\\top W_q X_q) \n",
    "$$\n",
    "Writing this in sparse terms,\n",
    "$$\n",
    "\\sqrt{n} \\alpha_{kq} = (W_k W_{xs} S_k)^\\top (W_q W_{xs} S_q) = S_k^\\top (W_{xs}^\\top W_k^\\top W_q W_{xs} S_q) = (S_k^\\top W_{xs}^\\top W_k^\\top W_q W_{xs}) S_q\n",
    "$$\n",
    "This means that when we fix $S_k$, we approximately get a linear function of $S_q$ (of course, except for the bias and scaling terms induced by the LayerNorm).\n",
    "\n",
    "Finally, we note that the values that get input to the MLP are $\\sum_{k} \\operatorname{Softmax}(\\alpha_{kq}) W_v X_v$ (if we assume $X_k = X_v$, as is the case in virtually every attention scheme, including cross-attention, the values are $\\sum_{k} \\operatorname{Softmax}(\\alpha_{kq}) W_v X_k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e2b29-fe7b-4f58-92ef-1bf3d48cf49b",
   "metadata": {},
   "source": [
    "Now let's try expanding this:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{k} \\operatorname{Softmax}(\\alpha_{kq}) W_v X_k\n",
    "    &= \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} (S_k^\\top W_{xs}^\\top W_k^\\top W_q W_{xs}) S_q\\right) W_v X_k \\\\\n",
    "    &= \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} (S_k^\\top W_{xs}^\\top W_k^\\top W_q W_{xs}) S_q\\right) W_v W_{xs} S_k \\\\\n",
    "    &= \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} (S_q^\\top W_{xs}^\\top W_q^\\top W_k W_{xs}) S_k\\right) W_v W_{xs} S_k\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c2e24-2e68-4387-aef3-584542052100",
   "metadata": {},
   "source": [
    "This gets added *back* to the residual stream, before being sent through another LayerNorm and eventually the MLP. So this is what MLP hidden state looks like (pre-activation):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MLPHiddenStatePreActivation}\n",
    " &= W_{mlp}^\\top \\left(W_{xs} S_q + \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} (S_q^\\top W_{xs}^\\top W_q^\\top W_k W_{xs}) S_k\\right) W_v W_{xs} S_k\\right) \\\\\n",
    " &= W_{mlp}^\\top W_{xs} S_q + \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} (S_q^\\top W_{xs}^\\top W_q^\\top W_k W_{xs}) S_k\\right) W_{mlp}^\\top W_v W_{xs} S_k\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba301f-662b-4bac-b246-7cd68d044777",
   "metadata": {},
   "source": [
    "OK, this looks great. We should be able to quantify the contribution to the activation of any given attention head given a sparse state. And the nice thing is that it looks very very pseudo-linear as a function of $S_q$ and $S_k$. The interesting thing is the $\\text{Softmax}$ term, which modulates the contribution of each other token. (Also, we're getting very very close to quantifying an MLP hidden neuron's activation as a function of prior sparse states!)\n",
    "\n",
    "Let's try to inspect the Softmax activation in a bit more detail, by constructing the matrix by which $S_q$ gets projected in order to match to $S_k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dccbfa9-60db-4668-b7e3-b1b368e77b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2_VOCAB_SIZE = gpt2.lm_head.weight.shape[0]\n",
    "GPT2_MAX_POS_EMB = gpt2.transformer.wpe.weight.shape[0]\n",
    "GPT2_VOCAB_SIZE, GPT2_MAX_POS_EMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1358175b-e1c1-40b4-ac0b-5a3a4a2197df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496]\n"
     ]
    }
   ],
   "source": [
    "# Let's say we have the token \"Hello\" and we want to see what activates in response.\n",
    "sparse_tensor = torch.zeros((GPT2_VOCAB_SIZE + GPT2_MAX_POS_EMB)).cuda()\n",
    "\n",
    "ids = gpt2_tokenizer(\"Hello\").input_ids\n",
    "print(ids)\n",
    "tok = ids[0]\n",
    "\n",
    "sparse_tensor[tok] = 1\n",
    "sparse_tensor[GPT2_VOCAB_SIZE + 0] = 1\n",
    "\n",
    "W_xs = torch.cat([gpt2.transformer.wte.weight, gpt2.transformer.wpe.weight], dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ac8d113-4724-4073-9804-9478a0d3ebf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_hidden_state = W_xs @ sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eec131e-e686-45ef-bb4b-1e7491732a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = gpt2(input_ids=torch.tensor([15496], device='cuda'), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80a3dbdb-f4f2-40e4-9fa3-da8cf99f0e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: 0.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Error: \" + str((output.hidden_states[0] - expected_hidden_state).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd59fe-61ab-468a-8aa7-e0e07cc96d0d",
   "metadata": {},
   "source": [
    "So we've constructed $W_{xs}$ and we now need $W_q$ and $W_k$ for the corresponding head, and $W_{mlp}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ace2f22b-a7cf-4894-8bf6-ea20ba62c09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, torch.Size([768, 2304]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 groups of 12 heads of dimension 64 (query, key, and value groups)\n",
    "gpt2.transformer.h[0].attn.head_dim, gpt2.transformer.h[0].attn.c_attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c34a416-7fb7-493b-bf2a-97754020a5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is used to split the result after matmul\n",
    "split_size = gpt2.transformer.h[0].attn.split_size\n",
    "head_dim = gpt2.transformer.h[0].attn.head_dim\n",
    "split_size, head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "923dd4f7-8d2e-44a5-ab0f-869d974e005b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qkv_weight = gpt2.transformer.h[0].attn.c_attn.weight\n",
    "# torch.split returns \"views\", which are more memory-efficient than creating slices I think\n",
    "query_weights, key_weights, value_weights = torch.split(qkv_weight, split_size, dim=-1)\n",
    "query_weights_per_head = torch.split(query_weights, head_dim, dim=-1)\n",
    "key_weights_per_head = torch.split(key_weights, head_dim, dim=-1)\n",
    "value_weights_per_head = torch.split(key_weights, head_dim, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94a71b-805e-4fd7-a0cc-9028df214d11",
   "metadata": {},
   "source": [
    "Let's give a name for the monster matrix that multiplies $S_q$. We'll just call it $M$ for \"match matrix\".\n",
    "$$\n",
    "\\begin{align*}\n",
    "    M &= W_{xs}^\\top W_k^\\top W_q W_{xs} \\\\\n",
    "    AttnScore &= Softmax(S_k^\\top M S_q)\n",
    "\\end{align*}\n",
    "$$\n",
    "Then $MS_q$ represents what the query is looking for in $S_k$ (for the given attention head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5009f9b-6a0c-4667-a426-23500481fc49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads = len(query_weights_per_head)\n",
    "M_per_head = [\n",
    "    W_xs.T @ key_weights_per_head[i] @ query_weights_per_head[i].T @ W_xs\n",
    "    for i in range(1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a611f183-78f6-434e-ab75-d5dc79e59fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51281, 51281])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_per_head[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fbc5e-6b5e-489e-8183-68182a0eec69",
   "metadata": {},
   "source": [
    "Unfortunately, $M$ also tends to be a monstrous matrix. BUT - it should tell us what the attention head is looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75873bbd-a26c-4404-9186-264b067ae00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30906, 23517)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = M_per_head[0]\n",
    "divmod(M.argmax().item(), M.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5c17f4e-908e-43e4-8a6b-5474ce7cd9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_val = M.max()\n",
    "results = []\n",
    "for i in range(M.shape[0]):\n",
    "    a = (M[i] == max_val).nonzero()\n",
    "    if len(a) > 0:\n",
    "        results.append((i, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f87effbc-ad55-4807-877c-c50932332c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = M.cpu().flatten().numpy()[::100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ddf0392-deb0-4759-ab47-20ce0244edf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diag = torch.diagonal(M).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8f6f1b5-7e14-4a78-8afa-3ecbddebc160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16440/1774407410.py:4: RuntimeWarning: invalid value encountered in log10\n",
      "  plt.hist(np.log10(diag + 1), bins=40)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5iklEQVR4nO3dfVhUdf7/8dcgAt7N4C3IL0RMK1FXTU3pxnIlMa2WKys1Vm0j/dYXLLUtdUvUds3SzJvVldzacHdrM0spsUURLdokRYy8SejOgvQ74IbMBCUizO+P1nM5ge4ZBWfQ5+O6znU15/OeM+/DXDWvPufMZywul8slAAAAnJOftxsAAABoCghNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwAR/bzdwqaitrdXRo0fVpk0bWSwWb7cDAABMcLlc+v777xUWFiY/v3PPJRGaGsjRo0cVHh7u7TYAAMB5KC4u1hVXXHHOGkJTA2nTpo2kn/7oVqvVy90AAAAznE6nwsPDjc/xcyE0NZDTl+SsViuhCQCAJsbMrTXcCA4AAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmODv7QYAAMCF6zprs0f1Xz87upE6uXQx0wQAAGACoQkAAMAEr4am7Oxs3XHHHQoLC5PFYlFaWtpZax966CFZLBYtW7bMbX9ZWZni4+NltVoVHByshIQEVVRUuNXs27dPN910k4KCghQeHq5FixbVOf769et1zTXXKCgoSH369NG7777bEKcIAAAuEV4NTZWVlerbt69WrVp1zrqNGzfqo48+UlhYWJ2x+Ph4HTx4UJmZmUpPT1d2dramTJlijDudTo0YMUIRERHKy8vT4sWLNW/ePK1Zs8ao2blzp8aPH6+EhAR9/PHHiouLU1xcnA4cONBwJwsAAJo0i8vlcnm7CUmyWCzauHGj4uLi3PYfOXJEgwcP1pYtWzR69GhNmzZN06ZNkyQdOnRIUVFRys3N1cCBAyVJGRkZGjVqlL799luFhYVp9erVevLJJ2W32xUQECBJmjVrltLS0lRQUCBJGjt2rCorK5Wenm687pAhQ9SvXz+lpKSY6t/pdMpms8nhcMhqtV7gXwMAAM9wI/j58eTz26fvaaqtrdWECRP0+OOPq1evXnXGc3JyFBwcbAQmSYqJiZGfn5927dpl1AwdOtQITJIUGxurwsJCHT9+3KiJiYlxO3ZsbKxycnLO2ltVVZWcTqfbBgAALl0+HZqee+45+fv765FHHql33G63q1OnTm77/P391a5dO9ntdqMmJCTEreb04/9Wc3q8PgsXLpTNZjO28PBwz04OAAA0KT4bmvLy8rR8+XKlpqbKYrF4u506Zs+eLYfDYWzFxcXebgkAADQinw1NH3zwgUpLS9WlSxf5+/vL399f33zzjR577DF17dpVkhQaGqrS0lK35506dUplZWUKDQ01akpKStxqTj/+bzWnx+sTGBgoq9XqtgEAgEuXz4amCRMmaN++fcrPzze2sLAwPf7449qyZYskKTo6WuXl5crLyzOet337dtXW1mrw4MFGTXZ2tqqrq42azMxMXX311Wrbtq1Rk5WV5fb6mZmZio6ObuzTBAAATYRXf0aloqJCX3zxhfH48OHDys/PV7t27dSlSxe1b9/erb558+YKDQ3V1VdfLUnq2bOnRo4cqcmTJyslJUXV1dVKSkrSuHHjjOUJ7rvvPs2fP18JCQmaOXOmDhw4oOXLl2vp0qXGcR999FHdfPPNWrJkiUaPHq3XX39de/bscVuWAAAAXN68OtO0Z88e9e/fX/3795ckzZgxQ/3791dycrLpY7z66qu65pprNHz4cI0aNUo33nijW9ix2WzaunWrDh8+rAEDBuixxx5TcnKy21pO119/vV577TWtWbNGffv21Ztvvqm0tDT17t274U4WAAA0aT6zTlNTxzpNAABvYp2m83PJrNMEAADgKwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEr4am7Oxs3XHHHQoLC5PFYlFaWpoxVl1drZkzZ6pPnz5q1aqVwsLCNHHiRB09etTtGGVlZYqPj5fValVwcLASEhJUUVHhVrNv3z7ddNNNCgoKUnh4uBYtWlSnl/Xr1+uaa65RUFCQ+vTpo3fffbdRzhkAADRNXg1NlZWV6tu3r1atWlVn7IcfftDevXs1Z84c7d27Vxs2bFBhYaHuvPNOt7r4+HgdPHhQmZmZSk9PV3Z2tqZMmWKMO51OjRgxQhEREcrLy9PixYs1b948rVmzxqjZuXOnxo8fr4SEBH388ceKi4tTXFycDhw40HgnDwAAmhSLy+VyebsJSbJYLNq4caPi4uLOWpObm6vrrrtO33zzjbp06aJDhw4pKipKubm5GjhwoCQpIyNDo0aN0rfffquwsDCtXr1aTz75pOx2uwICAiRJs2bNUlpamgoKCiRJY8eOVWVlpdLT043XGjJkiPr166eUlJR6e6mqqlJVVZXx2Ol0Kjw8XA6HQ1ar9UL/HAAAeKTrrM0e1X/97OhG6qRpcTqdstlspj6/m9Q9TQ6HQxaLRcHBwZKknJwcBQcHG4FJkmJiYuTn56ddu3YZNUOHDjUCkyTFxsaqsLBQx48fN2piYmLcXis2NlY5OTln7WXhwoWy2WzGFh4e3lCnCQAAfFCTCU0nTpzQzJkzNX78eCMJ2u12derUya3O399f7dq1k91uN2pCQkLcak4//m81p8frM3v2bDkcDmMrLi6+sBMEAAA+zd/bDZhRXV2te++9Vy6XS6tXr/Z2O5KkwMBABQYGersNAABwkfh8aDodmL755htt377d7XpjaGioSktL3epPnTqlsrIyhYaGGjUlJSVuNacf/7ea0+MAAAA+fXnudGD6/PPPtW3bNrVv395tPDo6WuXl5crLyzP2bd++XbW1tRo8eLBRk52drerqaqMmMzNTV199tdq2bWvUZGVluR07MzNT0dHRjXVqAACgifFqaKqoqFB+fr7y8/MlSYcPH1Z+fr6KiopUXV2tu+++W3v27NGrr76qmpoa2e122e12nTx5UpLUs2dPjRw5UpMnT9bu3bv14YcfKikpSePGjVNYWJgk6b777lNAQIASEhJ08OBBrVu3TsuXL9eMGTOMPh599FFlZGRoyZIlKigo0Lx587Rnzx4lJSVd9L8JAADwTV5dcuC9997TsGHD6uyfNGmS5s2bp8jIyHqft2PHDt1yyy2SflrcMikpSZs2bZKfn5/GjBmjFStWqHXr1kb9vn37lJiYqNzcXHXo0EFTp07VzJkz3Y65fv16PfXUU/r666/Vo0cPLVq0SKNGjTJ9Lp58ZREAgIbGkgPnx5PPb59Zp6mpIzQBALyJ0HR+Ltl1mgAAALyF0AQAAGACoQkAAMAEQhMAAIAJPr+4JQAAlytPb+5G42KmCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEr4am7Oxs3XHHHQoLC5PFYlFaWprbuMvlUnJysjp37qwWLVooJiZGn3/+uVtNWVmZ4uPjZbVaFRwcrISEBFVUVLjV7Nu3TzfddJOCgoIUHh6uRYsW1ell/fr1uuaaaxQUFKQ+ffro3XffbfDzBQAATZdXQ1NlZaX69u2rVatW1Tu+aNEirVixQikpKdq1a5datWql2NhYnThxwqiJj4/XwYMHlZmZqfT0dGVnZ2vKlCnGuNPp1IgRIxQREaG8vDwtXrxY8+bN05o1a4yanTt3avz48UpISNDHH3+suLg4xcXF6cCBA4138gAAoEmxuFwul7ebkCSLxaKNGzcqLi5O0k+zTGFhYXrsscf029/+VpLkcDgUEhKi1NRUjRs3TocOHVJUVJRyc3M1cOBASVJGRoZGjRqlb7/9VmFhYVq9erWefPJJ2e12BQQESJJmzZqltLQ0FRQUSJLGjh2ryspKpaenG/0MGTJE/fr1U0pKSr39VlVVqaqqynjsdDoVHh4uh8Mhq9Xa4H8fAMDlp+uszY127K+fHd1ox25KnE6nbDabqc9vn72n6fDhw7Lb7YqJiTH22Ww2DR48WDk5OZKknJwcBQcHG4FJkmJiYuTn56ddu3YZNUOHDjUCkyTFxsaqsLBQx48fN2rOfJ3TNadfpz4LFy6UzWYztvDw8As/aQAA4LN8NjTZ7XZJUkhIiNv+kJAQY8xut6tTp05u4/7+/mrXrp1bTX3HOPM1zlZzerw+s2fPlsPhMLbi4mJPTxEAADQh/t5uoKkKDAxUYGCgt9sAAAAXic/ONIWGhkqSSkpK3PaXlJQYY6GhoSotLXUbP3XqlMrKytxq6jvGma9xtprT4wAAAD4bmiIjIxUaGqqsrCxjn9Pp1K5duxQdHS1Jio6OVnl5ufLy8oya7du3q7a2VoMHDzZqsrOzVV1dbdRkZmbq6quvVtu2bY2aM1/ndM3p1wEAAPBqaKqoqFB+fr7y8/Ml/XTzd35+voqKimSxWDRt2jT94Q9/0DvvvKP9+/dr4sSJCgsLM75h17NnT40cOVKTJ0/W7t279eGHHyopKUnjxo1TWFiYJOm+++5TQECAEhISdPDgQa1bt07Lly/XjBkzjD4effRRZWRkaMmSJSooKNC8efO0Z88eJSUlXew/CQAA8FFevadpz549GjZsmPH4dJCZNGmSUlNT9cQTT6iyslJTpkxReXm5brzxRmVkZCgoKMh4zquvvqqkpCQNHz5cfn5+GjNmjFasWGGM22w2bd26VYmJiRowYIA6dOig5ORkt7Wcrr/+er322mt66qmn9Lvf/U49evRQWlqaevfufRH+CgAAoCk4r3WaqqurZbfb9cMPP6hjx45q165dY/TWpHiyzgMAAGawTlPja5R1mr7//nutXr1aN998s6xWq7p27aqePXuqY8eOioiI0OTJk5Wbm3vBzQMAAPgiU6HphRdeUNeuXfXKK68oJiZGaWlpys/P12effaacnBzNnTtXp06d0ogRIzRy5Mg6vw8HAADQ1Jm6pyk3N1fZ2dnq1atXvePXXXedHnjgAaWkpOiVV17RBx98oB49ejRoowAAAN5kKjT94x//MHWwwMBAPfTQQxfUEAAAgC867yUHfv6DtQAAAJcyj0JTZmamRo0apbZt26ply5Zq2bKl2rZtq1GjRmnbtm2N1SMAAIDXmQ5Na9eu1ahRo2Sz2bR06VKlp6crPT1dS5cuVXBwsEaNGqW//e1vjdkrAACA15he3HLBggVatmyZEhMT64zdf//9uvHGG/X0009rwoQJDdogAACALzA901RUVKSYmJizjg8fPlzffvttgzQFAADga0yHpl69eunll18+6/hf/vIXRUVFNUhTAAAAvsb05bklS5bo9ttvV0ZGhmJiYhQSEiJJKikpUVZWlr766itt3tx4y70DAAB4k+nQdMstt+jAgQNavXq1PvroI9ntdklSaGiobrvtNj300EPq2rVrY/UJAADgVaZDkyR17dpVzz33XGP1AgAA4LPOe3FLAACAy0mDhaZPPvlEzZo1a6jDAQAA+JQGnWlyuVwNeTgAAACfYfqeprvuuuuc4w6HQxaL5YIbAgAA8EWmQ9OmTZt06623GksN/FxNTU2DNQUAAOBrTIemnj17asyYMUpISKh3PD8/X+np6Q3WGAAAgC8xfU/TgAEDtHfv3rOOBwYGqkuXLg3SFAAAgK8xPdOUkpJyzktwPXv21OHDhxukKQAAAF9jOjQFBgY2Zh8AAAA+zaMVwSXJ6XTWu99isSgwMFABAQEX3BQAAICv8Tg0BQcHn3NpgSuuuEL333+/5s6dKz8/FhwHAACXBo9DU2pqqp588kndf//9uu666yRJu3fv1tq1a/XUU0/p2LFjev755xUYGKjf/e53Dd4wAACAN3gcmtauXaslS5bo3nvvNfbdcccd6tOnj1588UVlZWWpS5cuWrBgAaEJAABcMjy+frZz507179+/zv7+/fsrJydHknTjjTeqqKjowrsDAADwER6HpvDwcL388st19r/88ssKDw+XJH333Xdq27bthXcHAADgIzy+PPf888/rnnvu0T//+U8NGjRIkrRnzx4VFBTozTfflCTl5uZq7NixDdspAACAF3kcmu68804VFBToxRdf1GeffSZJuu2225SWlqauXbtKkh5++OEGbRIAAMDbPA5NkhQZGalnn322oXsBAADwWecVmsrLy/Xyyy/r0KFDkqRevXrpgQcekM1ma9DmAAAAfIXHN4Lv2bNHV155pZYuXaqysjKVlZXphRde0JVXXnnOH/QFAABoyjyeaZo+fbruvPNO/fnPf5a//09PP3XqlB588EFNmzZN2dnZDd4kAACAt3kcmvbs2eMWmCTJ399fTzzxhAYOHNigzQEAAPgKjy/PWa3WeheuLC4uVps2bRqkKQAAAF/jcWgaO3asEhIStG7dOhUXF6u4uFivv/66HnzwQY0fP74xegQAAPC681rc0mKxaOLEiTp16pQkqXnz5nr44YdZhgAAAFyyPJ5pCggI0PLly3X8+HHl5+crPz9fZWVlWrp0qQIDAxu0uZqaGs2ZM0eRkZFq0aKFrrzySv3+97+Xy+Uyalwul5KTk9W5c2e1aNFCMTEx+vzzz92OU1ZWpvj4eFmtVgUHByshIUEVFRVuNfv27dNNN92koKAghYeHa9GiRQ16LgAAoGnzODSd1rJlS/Xp00d9+vRRy5YtG7Inw3PPPafVq1dr5cqVOnTokJ577jktWrRIf/zjH42aRYsWacWKFUpJSdGuXbvUqlUrxcbG6sSJE0ZNfHy8Dh48qMzMTKWnpys7O1tTpkwxxp1Op0aMGKGIiAjl5eVp8eLFmjdvntasWdMo5wUAAJoei+vMaZuzuOuuu0wfcMOGDRfU0Jluv/12hYSEuP1A8JgxY9SiRQv9/e9/l8vlUlhYmB577DH99re/lSQ5HA6FhIQoNTVV48aN06FDhxQVFaXc3Fzj230ZGRkaNWqUvv32W4WFhWn16tV68sknZbfbFRAQIEmaNWuW0tLSVFBQYKpXp9Mpm80mh8Mhq9XaYH8DAMDlq+uszY127K+fHd1ox25KPPn8NjXTZLPZTG8N6frrr1dWVpbxG3effPKJ/vWvf+m2226TJB0+fFh2u10xMTFuvQ4ePFg5OTmSpJycHAUHB7sthxATEyM/Pz/t2rXLqBk6dKgRmCQpNjZWhYWFOn78eL29VVVVyel0um0AAODSZepG8FdeeaWx+6jXrFmz5HQ6dc0116hZs2aqqanRggULFB8fL0my2+2SpJCQELfnhYSEGGN2u12dOnVyG/f391e7du3caiIjI+sc4/RY27Zt6/S2cOFCzZ8/vwHOEgAANAXnfU/TxfDGG2/o1Vdf1Wuvvaa9e/dq7dq1ev7557V27Vpvt6bZs2fL4XAYW3FxsbdbAgAAjchUaBo5cqQ++uij/1r3/fff67nnntOqVasuuDFJevzxxzVr1iyNGzdOffr00YQJEzR9+nQtXLhQkhQaGipJKikpcXteSUmJMRYaGqrS0lK38VOnTqmsrMytpr5jnPkaPxcYGCir1eq2AQCAS5ep0HTPPfdozJgxioqK0syZM7V+/Xp9+OGHysvL07Zt27RixQrde++96ty5s/bu3as77rijQZr74Ycf5Ofn3mKzZs1UW1srSYqMjFRoaKiysrKMcafTqV27dik6OlqSFB0drfLycuXl5Rk127dvV21trQYPHmzUZGdnq7q62qjJzMzU1VdfXe+lOQAAcPkxdU9TQkKCfv3rX2v9+vVat26d1qxZI4fDIUmyWCyKiopSbGyscnNz1bNnzwZr7o477tCCBQvUpUsX9erVSx9//LFeeOEFPfDAA8ZrT5s2TX/4wx/Uo0cPRUZGas6cOQoLC1NcXJwkqWfPnho5cqQmT56slJQUVVdXKykpSePGjVNYWJgk6b777tP8+fOVkJCgmTNn6sCBA1q+fLmWLl3aYOcCAACaNlNLDtTH4XDoxx9/VPv27dW8efOG7kvST5f75syZo40bN6q0tFRhYWEaP368kpOTjW+6uVwuzZ07V2vWrFF5ebluvPFG/elPf9JVV11lHKesrExJSUnatGmT/Pz8NGbMGK1YsUKtW7c2avbt26fExETl5uaqQ4cOmjp1qmbOnGm6V5YcAAA0NJYcaHyefH6fd2iCO0ITAKChEZoanyef3x7/9hwAAGj6PAlkBKyf+PSSAwAAAL6C0AQAAGACoQkAAMAEj0PTpEmTlJ2d3Ri9AAAA+CyPQ5PD4VBMTIx69OihZ555RkeOHGmMvgAAAHyKx6EpLS1NR44c0cMPP6x169apa9euuu222/Tmm2+6ragNAABwKTmve5o6duyoGTNm6JNPPtGuXbvUvXt3TZgwQWFhYZo+fbo+//zzhu4TAADAqy7oRvD/+7//U2ZmpjIzM9WsWTONGjVK+/fvV1RUFD9BAgAALikeh6bq6mq99dZbuv322xUREaH169dr2rRpOnr0qNauXatt27bpjTfe0NNPP90Y/QIAAHiFxyuCd+7cWbW1tRo/frx2796tfv361akZNmyYgoODG6A9AAAA3+BxaFq6dKnuueceBQUFnbUmODhYhw8fvqDGAAAAfInHl+d27NhR77fkKisr9cADDzRIUwAAAL7G49C0du1a/fjjj3X2//jjj/rrX//aIE0BAAD4GtOX55xOp1wul1wul77//nu3y3M1NTV699131alTp0ZpEgAAwNtMh6bg4GBZLBZZLBZdddVVdcYtFovmz5/foM0BAAD4CtOhaceOHXK5XPrlL3+pt956S+3atTPGAgICFBERobCwsEZpEgAAwNtMh6abb75ZknT48GF16dJFFoul0ZoCAADwNaZC0759+9S7d2/5+fnJ4XBo//79Z639xS9+0WDNAQAA+ApToalfv36y2+3q1KmT+vXrJ4vFIpfLVafOYrGopqamwZsEAADwNlOh6fDhw+rYsaPxzwAAAJcbU6EpIiKi3n8GAAC4XHj8Mypr165Vhw4dNHr0aEnSE088oTVr1igqKkr/+Mc/CFUAAJxD11mbvd0CzpPHK4I/88wzatGihSQpJydHK1eu1KJFi9ShQwdNnz69wRsEAADwBR7PNBUXF6t79+6SpLS0NN19992aMmWKbrjhBt1yyy0N3R8AAIBP8HimqXXr1vruu+8kSVu3btWtt94qSQoKCqr3N+kAAAAuBR7PNN1666168MEH1b9/f3322WcaNWqUJOngwYPq2rVrQ/cHAADgEzyeaVq1apWio6N17NgxvfXWW2rfvr0kKS8vT+PHj2/wBgEAAHyBxzNNwcHBWrlyZZ39/FgvAAC4lHkcmiSpvLxcu3fvVmlpqWpra439FotFEyZMaLDmAAAAfIXHoWnTpk2Kj49XRUWFrFar2w/3EpoAAMClyuN7mh577DE98MADqqioUHl5uY4fP25sZWVljdEjAACA13kcmo4cOaJHHnlELVu2bIx+AAAAfJLHoSk2NlZ79uxpjF4AAAB8lsf3NI0ePVqPP/64Pv30U/Xp00fNmzd3G7/zzjsbrDkAAABf4XFomjx5siTp6aefrjNmsVhUU1Nz4V0BAAD4GI9D05lLDAAAAFwuPL6n6UwnTpxoqD4AAAB8msehqaamRr///e/1//7f/1Pr1q311VdfSZLmzJmjl19+ucEbPHLkiH7961+rffv2atGihfr06eN2I7rL5VJycrI6d+6sFi1aKCYmRp9//rnbMcrKyhQfHy+r1arg4GAlJCSooqLCrWbfvn266aabFBQUpPDwcC1atKjBzwUAADRdHoemBQsWKDU1VYsWLVJAQICxv3fv3nrppZcatLnjx4/rhhtuUPPmzfXPf/5Tn376qZYsWaK2bdsaNYsWLdKKFSuUkpKiXbt2qVWrVoqNjXWbBYuPj9fBgweVmZmp9PR0ZWdna8qUKca40+nUiBEjFBERoby8PC1evFjz5s3TmjVrGvR8AABA02VxuVwuT57QvXt3vfjiixo+fLjatGmjTz75RN26dVNBQYGio6N1/PjxBmtu1qxZ+vDDD/XBBx/UO+5yuRQWFqbHHntMv/3tbyVJDodDISEhSk1N1bhx43To0CFFRUUpNzdXAwcOlCRlZGRo1KhR+vbbbxUWFqbVq1frySeflN1uN4LgrFmzlJaWpoKCAlO9Op1O2Ww2ORwOWa3WBjh7AMClqOuszd5uwWNfPzva2y00Gk8+v89rccvu3bvX2V9bW6vq6mpPD3dO77zzjgYOHKh77rlHnTp1Uv/+/fXnP//ZGD98+LDsdrtiYmKMfTabTYMHD1ZOTo4kKScnR8HBwUZgkqSYmBj5+flp165dRs3QoUPdZs5iY2NVWFh41hBYVVUlp9PptgEAgEuXx6EpKiqq3pmfN998U/3792+Qpk776quvtHr1avXo0UNbtmzRww8/rEceeURr166VJNntdklSSEiI2/NCQkKMMbvdrk6dOrmN+/v7q127dm419R3jzNf4uYULF8pmsxlbeHj4BZ4tAADwZR4vOZCcnKxJkybpyJEjqq2t1YYNG1RYWKi//vWvSk9Pb9DmamtrNXDgQD3zzDOSpP79++vAgQNKSUnRpEmTGvS1PDV79mzNmDHDeOx0OglOAABcwjyeafrVr36lTZs2adu2bWrVqpWSk5N16NAhbdq0SbfeemuDNte5c2dFRUW57evZs6eKiookSaGhoZKkkpISt5qSkhJjLDQ0VKWlpW7jp06dUllZmVtNfcc48zV+LjAwUFar1W0DAACXrvNap+mmm25SZmamSktL9cMPP+hf//qXRowY0dC96YYbblBhYaHbvs8++0wRERGSpMjISIWGhiorK8sYdzqd2rVrl6KjoyVJ0dHRKi8vV15enlGzfft21dbWavDgwUZNdna22z1ZmZmZuvrqq92+qQcAAC5fHoembt266bvvvquzv7y8XN26dWuQpk6bPn26PvroIz3zzDP64osv9Nprr2nNmjVKTEyU9NPPtkybNk1/+MMf9M4772j//v2aOHGiwsLCFBcXJ+mnmamRI0dq8uTJ2r17tz788EMlJSVp3LhxCgsLkyTdd999CggIUEJCgg4ePKh169Zp+fLlbpffAADA5c3je5q+/vrren9frqqqSkeOHGmQpk4bNGiQNm7cqNmzZ+vpp59WZGSkli1bpvj4eKPmiSeeUGVlpaZMmaLy8nLdeOONysjIUFBQkFHz6quvKikpScOHD5efn5/GjBmjFStWGOM2m01bt25VYmKiBgwYoA4dOig5OdltLScAAHB5M71O0zvvvCNJiouL09q1a2Wz2YyxmpoaZWVlKTMzs87ltMsF6zQBAMxgnSbf4snnt+mZptOXuywWS51vrjVv3lxdu3bVkiVLPO8WAACgCTAdmmprayX9dPN1bm6uOnTo0GhNAQAA+BqP72k6fPhwY/QBAADg0zwOTZKUlZWlrKwslZaWGjNQp/3lL39pkMYAAAB8icehaf78+Xr66ac1cOBAde7cWRaLpTH6AgAA8Ckeh6aUlBSlpqZqwoQJjdEPAACAT/J4ccuTJ0/q+uuvb4xeAAAAfJbHoenBBx/Ua6+91hi9AAAA+CyPL8+dOHFCa9as0bZt2/SLX/xCzZs3dxt/4YUXGqw5AAAAX+FxaNq3b5/69esnSTpw4IDbGDeFAwCAS5XHoWnHjh2N0QcAAIBP8/ieJgAAgMuR6Zmmu+66y1Tdhg0bzrsZAAAAX2U6NNlstsbsAwAAwKeZDk2vvPJKY/YBAADg07inCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYEKTCk3PPvusLBaLpk2bZuw7ceKEEhMT1b59e7Vu3VpjxoxRSUmJ2/OKioo0evRotWzZUp06ddLjjz+uU6dOudW89957uvbaaxUYGKju3bsrNTX1IpwRAABoKppMaMrNzdWLL76oX/ziF277p0+frk2bNmn9+vV6//33dfToUd11113GeE1NjUaPHq2TJ09q586dWrt2rVJTU5WcnGzUHD58WKNHj9awYcOUn5+vadOm6cEHH9SWLVsu2vkBAADf1iRCU0VFheLj4/XnP/9Zbdu2NfY7HA69/PLLeuGFF/TLX/5SAwYM0CuvvKKdO3fqo48+kiRt3bpVn376qf7+97+rX79+uu222/T73/9eq1at0smTJyVJKSkpioyM1JIlS9SzZ08lJSXp7rvv1tKlS8/aU1VVlZxOp9sGAAAuXU0iNCUmJmr06NGKiYlx25+Xl6fq6mq3/ddcc426dOminJwcSVJOTo769OmjkJAQoyY2NlZOp1MHDx40an5+7NjYWOMY9Vm4cKFsNpuxhYeHX/B5AgAA3+Xzoen111/X3r17tXDhwjpjdrtdAQEBCg4OdtsfEhIiu91u1JwZmE6Pnx47V43T6dSPP/5Yb1+zZ8+Ww+EwtuLi4vM6PwAA0DT4e7uBcykuLtajjz6qzMxMBQUFebsdN4GBgQoMDPR2GwAA4CLx6ZmmvLw8lZaW6tprr5W/v7/8/f31/vvva8WKFfL391dISIhOnjyp8vJyt+eVlJQoNDRUkhQaGlrn23SnH/+3GqvVqhYtWjTS2QEAgKbEp0PT8OHDtX//fuXn5xvbwIEDFR8fb/xz8+bNlZWVZTynsLBQRUVFio6OliRFR0dr//79Ki0tNWoyMzNltVoVFRVl1Jx5jNM1p48BAADg05fn2rRpo969e7vta9Wqldq3b2/sT0hI0IwZM9SuXTtZrVZNnTpV0dHRGjJkiCRpxIgRioqK0oQJE7Ro0SLZ7XY99dRTSkxMNC6vPfTQQ1q5cqWeeOIJPfDAA9q+fbveeOMNbd68+eKeMAAA8Fk+HZrMWLp0qfz8/DRmzBhVVVUpNjZWf/rTn4zxZs2aKT09XQ8//LCio6PVqlUrTZo0SU8//bRRExkZqc2bN2v69Olavny5rrjiCr300kuKjY31xikBAAAfZHG5XC5vN3EpcDqdstlscjgcslqt3m4HAOCjus5qelcxvn52tLdbaDSefH43+ZkmAAC8qSmGIJwfn74RHAAAwFcQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABggk+HpoULF2rQoEFq06aNOnXqpLi4OBUWFrrVnDhxQomJiWrfvr1at26tMWPGqKSkxK2mqKhIo0ePVsuWLdWpUyc9/vjjOnXqlFvNe++9p2uvvVaBgYHq3r27UlNTG/v0AABAE+LToen9999XYmKiPvroI2VmZqq6ulojRoxQZWWlUTN9+nRt2rRJ69ev1/vvv6+jR4/qrrvuMsZramo0evRonTx5Ujt37tTatWuVmpqq5ORko+bw4cMaPXq0hg0bpvz8fE2bNk0PPvigtmzZclHPFwAA+C6Ly+VyebsJs44dO6ZOnTrp/fff19ChQ+VwONSxY0e99tpruvvuuyVJBQUF6tmzp3JycjRkyBD985//1O23366jR48qJCREkpSSkqKZM2fq2LFjCggI0MyZM7V582YdOHDAeK1x48apvLxcGRkZ9fZSVVWlqqoq47HT6VR4eLgcDoesVmsj/hUAAI2t66zN3m7Bp3z97Ghvt9BonE6nbDabqc9vn55p+jmHwyFJateunSQpLy9P1dXViomJMWquueYadenSRTk5OZKknJwc9enTxwhMkhQbGyun06mDBw8aNWce43TN6WPUZ+HChbLZbMYWHh7eMCcJAAB8UpMJTbW1tZo2bZpuuOEG9e7dW5Jkt9sVEBCg4OBgt9qQkBDZ7Xaj5szAdHr89Ni5apxOp3788cd6+5k9e7YcDoexFRcXX/A5AgAA3+Xv7QbMSkxM1IEDB/Svf/3L261IkgIDAxUYGOjtNgAAwEXSJGaakpKSlJ6erh07duiKK64w9oeGhurkyZMqLy93qy8pKVFoaKhR8/Nv051+/N9qrFarWrRo0dCnAwAAmiCfDk0ul0tJSUnauHGjtm/frsjISLfxAQMGqHnz5srKyjL2FRYWqqioSNHR0ZKk6Oho7d+/X6WlpUZNZmamrFaroqKijJozj3G65vQxAAAAfPryXGJiol577TW9/fbbatOmjXEPks1mU4sWLWSz2ZSQkKAZM2aoXbt2slqtmjp1qqKjozVkyBBJ0ogRIxQVFaUJEyZo0aJFstvteuqpp5SYmGhcXnvooYe0cuVKPfHEE3rggQe0fft2vfHGG9q8mW9PAACAn/j0TNPq1avlcDh0yy23qHPnzsa2bt06o2bp0qW6/fbbNWbMGA0dOlShoaHasGGDMd6sWTOlp6erWbNmio6O1q9//WtNnDhRTz/9tFETGRmpzZs3KzMzU3379tWSJUv00ksvKTY29qKeLwAA8F1Nap0mX+bJOg8AAN/GOk3uWKfpJz490wQAAOArCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATPD3dgMAAFwM/J4cLhQzTQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYwG/PAQCaLH5PDhcTM00AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAt+eAwAA5+TJtxS/fnZ0I3biXYQmAIDPYAkB+DIuzwEAAJhAaAIAADCBy3MAgEbFJTdcKphpAgAAMIHQBAAAYAKX5wAAHuOSGy5HzDQBAACYwEzTz6xatUqLFy+W3W5X37599cc//lHXXXedt9sCgEbH7BFwboSmM6xbt04zZsxQSkqKBg8erGXLlik2NlaFhYXq1KmTt9sDAIIN4EUWl8vl8nYTvmLw4MEaNGiQVq5cKUmqra1VeHi4pk6dqlmzZp3zuU6nUzabTQ6HQ1ar9WK0C+ASQRDCpaSp/YyKJ5/fzDT9x8mTJ5WXl6fZs2cb+/z8/BQTE6OcnJw69VVVVaqqqjIeOxwOST/98YFLRe+5W0zXHpgf2yjHBdC0dJm+3nStJ//daCynP7fNzCERmv7j3//+t2pqahQSEuK2PyQkRAUFBXXqFy5cqPnz59fZHx4e3mg9Ar7MtszbHQBoanzpvxvff/+9bDbbOWsITedp9uzZmjFjhvG4trZWZWVlat++vSwWixc7w7k4nU6Fh4eruLiYy6g+jveq6eC9ahp4n+rncrn0/fffKyws7L/WEpr+o0OHDmrWrJlKSkrc9peUlCg0NLROfWBgoAIDA932BQcHN2aLaEBWq5X/aDQRvFdNB+9V08D7VNd/m2E6jXWa/iMgIEADBgxQVlaWsa+2tlZZWVmKjo72YmcAAMAXMNN0hhkzZmjSpEkaOHCgrrvuOi1btkyVlZX6zW9+4+3WAACAlxGazjB27FgdO3ZMycnJstvt6tevnzIyMurcHI6mKzAwUHPnzq1zaRW+h/eq6eC9ahp4ny4c6zQBAACYwD1NAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQhMvanXfeqS5duigoKEidO3fWhAkTdPToUW+3hTN8/fXXSkhIUGRkpFq0aKErr7xSc+fO1cmTJ73dGuqxYMECXX/99WrZsiUL/vqYVatWqWvXrgoKCtLgwYO1e/dub7fU5BCacFkbNmyY3njjDRUWFuqtt97Sl19+qbvvvtvbbeEMBQUFqq2t1YsvvqiDBw9q6dKlSklJ0e9+9ztvt4Z6nDx5Uvfcc48efvhhb7eCM6xbt04zZszQ3LlztXfvXvXt21exsbEqLS31dmtNCksOAGd45513FBcXp6qqKjVv3tzb7eAsFi9erNWrV+urr77ydis4i9TUVE2bNk3l5eXebgWSBg8erEGDBmnlypWSfvrFi/DwcE2dOlWzZs3ycndNBzNNwH+UlZXp1Vdf1fXXX09g8nEOh0Pt2rXzdhtAk3Dy5Enl5eUpJibG2Ofn56eYmBjl5OR4sbOmh9CEy97MmTPVqlUrtW/fXkVFRXr77be93RLO4YsvvtAf//hH/c///I+3WwGahH//+9+qqamp8+sWISEhstvtXuqqaSI04ZIza9YsWSyWc24FBQVG/eOPP66PP/5YW7duVbNmzTRx4kRx1brxefo+SdKRI0c0cuRI3XPPPZo8ebKXOr/8nM97BVyKuKcJl5xjx47pu+++O2dNt27dFBAQUGf/t99+q/DwcO3cuVPR0dGN1SLk+ft09OhR3XLLLRoyZIhSU1Pl58f/810s5/PvFPc0+Y6TJ0+qZcuWevPNNxUXF2fsnzRpksrLy5ld9wA/2ItLTseOHdWxY8fzem5tba0kqaqqqiFbQj08eZ+OHDmiYcOGacCAAXrllVcITBfZhfw7Be8LCAjQgAEDlJWVZYSm2tpaZWVlKSkpybvNNTGEJly2du3apdzcXN14441q27atvvzyS82ZM0dXXnkls0w+5MiRI7rlllsUERGh559/XseOHTPGQkNDvdgZ6lNUVKSysjIVFRWppqZG+fn5kqTu3burdevW3m3uMjZjxgxNmjRJAwcO1HXXXadly5apsrJSv/nNb7zdWpNCaMJlq2XLltqwYYPmzp2ryspKde7cWSNHjtRTTz2lwMBAb7eH/8jMzNQXX3yhL774QldccYXbGHcX+J7k5GStXbvWeNy/f39J0o4dO3TLLbd4qSuMHTtWx44dU3Jysux2u/r166eMjIw6N4fj3LinCQAAwARuDAAAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCABMsFovS0tJ85jgALj5CEwCfZLfbNXXqVHXr1k2BgYEKDw/XHXfcoaysLG+3Zsq8efPUr1+/Ovv/7//+T7fddtvFbwjABeO35wD4nK+//lo33HCDgoODtXjxYvXp00fV1dXasmWLEhMTVVBQ4PExT548qYCAgDr7q6ur1bx584Zo2xR+ZBhouphpAuBz/vd//1cWi0W7d+/WmDFjdNVVV6lXr16aMWOGPvroI0lSUVGRfvWrX6l169ayWq269957VVJSYhzj9EzPSy+9pMjISAUFBUn66fLY6tWrdeedd6pVq1ZasGCBJOntt9/Wtddeq6CgIHXr1k3z58/XqVOnztrjzJkzddVVV6lly5bq1q2b5syZo+rqaklSamqq5s+fr08++UQWi0UWi0WpqanG6595eW7//v365S9/qRYtWqh9+/aaMmWKKioqjPH7779fcXFxev7559W5c2e1b99eiYmJxmsBuHiYaQLgU8rKypSRkaEFCxaoVatWdcaDg4NVW1trBKb3339fp06dUmJiosaOHav33nvPqP3iiy/01ltvacOGDWrWrJmxf968eXr22We1bNky+fv764MPPtDEiRO1YsUK3XTTTfryyy81ZcoUSdLcuXPr7bNNmzZKTU1VWFiY9u/fr8mTJ6tNmzZ64oknNHbsWB04cEAZGRnatm2bJMlms9U5RmVlpWJjYxUdHa3c3FyVlpbqwQcfVFJSkhGyJGnHjh3q3LmzduzYoS+++EJjx45Vv379NHny5PP5EwM4Xy4A8CG7du1ySXJt2LDhrDVbt251NWvWzFVUVGTsO3jwoEuSa/fu3S6Xy+WaO3euq3nz5q7S0lK350pyTZs2zW3f8OHDXc8884zbvr/97W+uzp07uz1v48aNZ+1p8eLFrgEDBhiP586d6+rbt2+dujOPs2bNGlfbtm1dFRUVxvjmzZtdfn5+Lrvd7nK5XK5Jkya5IiIiXKdOnTJq7rnnHtfYsWPP2guAxsFMEwCf4nK5/mvNoUOHFB4ervDwcGNfVFSUgoODdejQIQ0aNEiSFBERoY4dO9Z5/sCBA90ef/LJJ/rwww+NS3WSVFNToxMnTuiHH35Qy5Yt6xxj3bp1WrFihb788ktVVFTo1KlTslqtps/z9Hn07dvXbUbthhtuUG1trQoLCxUSEiJJ6tWrl9tMWefOnbV//36PXgvAhSM0AfApPXr0kMViOa+bvX+uvst79e2vqKjQ/Pnzddddd9WpPX0v1JlycnIUHx+v+fPnKzY2VjabTa+//rqWLFlywT3X5+c3qlssFtXW1jbKawE4O0ITAJ/Srl07xcbGatWqVXrkkUfqBJzy8nL17NlTxcXFKi4uNmabPv30U5WXlysqKsrj17z22mtVWFio7t27m6rfuXOnIiIi9OSTTxr7vvnmG7eagIAA1dTUnPM4PXv2VGpqqiorK43z/PDDD+Xn56err77aw7MA0Nj49hwAn7Nq1SrV1NTouuuu01tvvaXPP/9chw4d0ooVKxQdHa2YmBj16dNH8fHx2rt3r3bv3q2JEyfq5ptvrnPpzYzk5GT99a9/1fz583Xw4EEdOnRIr7/+up566ql663v06KGioiK9/vrr+vLLL7VixQpt3LjRraZr1646fPiw8vPz9e9//1tVVVV1jhMfH6+goCBNmjRJBw4c0I4dOzR16lRNmDDBuDQHwHcQmgD4nG7dumnv3r0aNmyYHnvsMfXu3Vu33nqrsrKytHr1alksFr399ttq27athg4dqpiYGHXr1k3r1q07r9eLjY1Venq6tm7dqkGDBmnIkCFaunSpIiIi6q2/8847NX36dCUlJalfv37auXOn5syZ41YzZswYjRw5UsOGDVPHjh31j3/8o85xWrZsqS1btqisrEyDBg3S3XffreHDh2vlypXndR4AGpfFZeauSwAAgMscM00AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmPD/Ae5+1RgGiyXSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(np.log10(diag + 1), bins=40)\n",
    "plt.ylabel(\"Intensity (log10)\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc57770e-e75f-4a1c-b8bb-2423ba4682b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4673]\n",
      "> therapist 0.94\n",
      "> DEBUG 0.92\n",
      ">MRI 0.92\n",
      "> neuroscience 0.91\n",
      "> neuro 0.89\n",
      "> ADHD 0.87\n",
      "> psychiatrists 0.86\n",
      "> Neurolog 0.86\n",
      "> psychiatrist 0.85\n",
      "> impair 0.85\n"
     ]
    }
   ],
   "source": [
    "ids = gpt2_tokenizer(' learning').input_ids\n",
    "print(ids)\n",
    "tok = ids[0]\n",
    "# represents multiplying by S_q, a matrix with the word \" learning\" at position 100.\n",
    "searched_by_this = M[:, tok] + M[:, GPT2_VOCAB_SIZE + 100]\n",
    "ranked = searched_by_this[:GPT2_VOCAB_SIZE].argsort(descending=True)\n",
    "for i in range(10):\n",
    "    print(f\">{gpt2_tokenizer.decode(ranked[i])} {searched_by_this[ranked[i]]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be413040-15da-4e12-ac6c-71262542e59e",
   "metadata": {},
   "source": [
    "If we put an exclamation mark, it seems to search for words that you might find in an exclamation-mark setting. Also it seems like each head searches for several correlations simultaneously.\n",
    "\n",
    "So it seems like we can inject a word and a positional embedding to find out what is attended to, and the nice thing is that we can do all of this without ever even needing to consider the residual stream whatsoever. Also, there’s the incredibly surprising constraint that this is a completely linear function, so the contributions from the word used, and the contributions from the position of the word, are treated completely independently.\n",
    "\n",
    "Bringing back from earlier:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MLPHiddenStatePreActivation}\n",
    " &= W_{mlp}^\\top W_{xs} S_q + \\sum_{k} \\operatorname{Softmax}\\left(n^{-1/2} \\cdot (MS_q)^\\top S_k\\right) W_{mlp}^\\top W_v W_{xs} S_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we can think of this as weighting the value matrices in a certain way.\n",
    "If we maintain $S_k$ separately, we note that $MLP^{(i)} = \\sum_{k} AttnScore(S_k, S_q) \\left((W_{mlp}^{(i)})^\\top W_v W_{xs} \\right) S_k$\n",
    "\n",
    "This shows that indeed each MLP hidden neuron is just a linear function of $AttnScore(S_k, S_q) S_k$. This is, in some sense, the \"kernel\" of that neuron, wrt a given query ($S_q$). Why is it nice for this to be a linear function? It means that which features the MLP neurons attend to (in $S_k$) can be precomputed, and then weighted AFTERWARDS. In my opinion, this is *highly unintuitive* and represents an extreme case of independent features in the transformer.\n",
    "\n",
    "Let me clear once more: The MLP neuron's activation proportion for a particular value vector, is INITIALLY NOT DEPENDENT ON THE QUERY VECTOR WHATSOEVER (until scaled in attention weights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1881b-1eb6-4645-83fa-89e442e555a8",
   "metadata": {},
   "source": [
    "Particularly, $\\nabla_{S_k} \\left((W_{mlp}^{(i)})^\\top W_v W_{xs} S_k \\right) = W_{xs}^\\top W_v^\\top W_{mlp}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a81b0c75-7754-40ce-97dc-b7e994890110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W_mlp_per_head = torch.split(gpt2.transformer.h[0].mlp.c_fc.weight, head_dim, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1587f4c-1490-4bc1-b563-ad22d12c4496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3072])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_mlp_per_head[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7c17b1b-b782-4fe6-a408-e8f2921c717d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = (W_xs.T @ value_weights_per_head[0] @ W_mlp_per_head[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "732665e0-fe99-4804-9c2f-49c894604d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Words :::\n",
      ">achy 1.13\n",
      "> syndrome 1.08\n",
      "> epilepsy 1.08\n",
      "> CBD 1.08\n",
      "> Clinical 1.08\n",
      ">EStreamFrame 1.06\n",
      ">uckland 1.06\n",
      "> nerv 1.05\n",
      "> clinical 1.05\n",
      "> neuron 1.04\n",
      "\n",
      "::: Positions :::\n",
      "pos 84: 0.30\n",
      "pos 78: 0.30\n",
      "pos 80: 0.30\n",
      "pos 79: 0.30\n",
      "pos 86: 0.29\n",
      "pos 73: 0.29\n",
      "pos 82: 0.29\n",
      "pos 96: 0.29\n",
      "pos 92: 0.29\n",
      "pos 88: 0.29\n"
     ]
    }
   ],
   "source": [
    "# What sparse features does MLP neuron 6 look for?\n",
    "neuron_num = 6\n",
    "ranked = target[:, neuron_num][:GPT2_VOCAB_SIZE].argsort(descending=True)\n",
    "print(\"::: Words :::\")\n",
    "for i in range(10):\n",
    "    print(f\">{gpt2_tokenizer.decode(ranked[i])} {target[:, neuron_num][ranked[i]]:.2f}\")\n",
    "print()\n",
    "print(\"::: Positions :::\")\n",
    "ranked = target[:, neuron_num][GPT2_VOCAB_SIZE:].argsort(descending=True)\n",
    "for i in range(10):\n",
    "    print(f\"pos {ranked[i]}: {target[:, neuron_num][GPT2_VOCAB_SIZE + ranked[i]]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2d4c1-d200-447f-b6be-f22192188746",
   "metadata": {},
   "source": [
    "# Step 3: Construct a Causal Trace\n",
    "\n",
    "We would like to see which MLP activations are responsible for which other MLP activations. For example, the token \"Hello\" predicting the token \" world\".\n",
    "\n",
    "To do this, we can look at the unembedding for the word \" world\" and see which MLP activations contribute the most. Is this invariant to the combination of activations that are provided? Well, let $e^{-1}$ be the unembedding for the word \" world\". Then the activation of world $\\log P(world|\\ldots)$ can be written as $e^{-1} \\text{ResidualStream}$, which is (up to a single LayerNorm applied at the end) linear w.r.t. all MLP activations (which contribute their own little vectors towards the residual stream). So we will always have the LayerNorm caveat (If we find a better way to ensure that hidden states don't explode, we can also try finetuning an existing nn towards that).\n",
    "\n",
    "The interesting thing is that MLP activations, word embeddings, and positional embeddings are the ONLY thing impacting the output for \" world\", and they can each be inspected individually.\n",
    "\n",
    "Maybe we can call this the \"sparse activation kernel\" for the token \" world\".\n",
    "\n",
    "(Also, I have a hunch that because most transformers use ReLU-style activations, the nonlinearities actually won't be too hurtful in most cases).\n",
    "\n",
    "An unanswered question I have is that in huggingface's GPT-2 implementation, the layernorm doesn't actually modify the residual stream every time; it's only used at final layers. I guess this is equivalent, though, or if there are slight variations it's easy to transpose these into the same problem.\n",
    "\n",
    "Also, here is a potential way to define the sparse space of a token's feature vector: It is the set of all possible vectors that can be added to the residual stream. This encapsulates the notion of word embeddings and positional embeddings being added to the residual stream, as well as MLP activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daea4ce5-f54a-4a5c-8878-87dfd31f79cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.h[i].mlp.c_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1154b384-870f-46ba-8084-e671ae58a9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find the sparse activation kernel for the token \"world\" \n",
    "@torch.no_grad()\n",
    "def get_sparse_activation_kernel(gpt2, target_vector, until_layer=12):\n",
    "    hidden_kernels = []\n",
    "    for i in range(until_layer):\n",
    "        hidden_kernels.append(\n",
    "            gpt2.transformer.h[i].mlp.c_proj.weight @ target_vector\n",
    "        )\n",
    "    # weight shapes: (50257 [GPT2_VOCAB_SIZE], 768 [d_model])\n",
    "    word_embedding_kernel = gpt2.transformer.wte.weight @ target_vector\n",
    "    pos_embedding_kernel = gpt2.transformer.wpe.weight @ target_vector\n",
    "    \n",
    "    return (word_embedding_kernel, pos_embedding_kernel, hidden_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cefaffb5-907f-41b3-b9c1-90890abdde20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2117d10e-c928-4ad6-998d-72d82d391d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world_tok = gpt2_tokenizer(' world').input_ids[0]\n",
    "hello_tok = gpt2_tokenizer('Hello').input_ids[0]\n",
    "act_kernel = get_sparse_activation_kernel(gpt2, gpt2.lm_head.weight[world_tok])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8a3d4-0e7e-43c3-89d5-2cd8ae539d36",
   "metadata": {},
   "source": [
    "## Plotting kernels\n",
    "\n",
    "Here, we plot the \"positional\" kernel -- the direct contribution of each positional encoding to the word \"world\"'s generation likelihood. I am surprised by how smooth it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c5b85f4-61bb-4d6e-917a-fdd01ee96de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot positional kernel\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_embedding_kernel, pos_embedding_kernel, hidden_kernels = act_kernel\n",
    "\n",
    "plt.title(\"Positional embedding generation kernel for ' world'\")\n",
    "plt.plot(pos_embedding_kernel.detach().cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1607cd2f-cf0b-47b8-8f08-c38cecda64ae",
   "metadata": {},
   "source": [
    "Now let's look at what words are most correlated with generating ' world'. We see a range of words that seem to be semantically similar (which I guess is what we might expect at the highest level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c9d48ab-0eb9-4376-b605-977b7a4bb6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Words :::\n",
      "> world 20.26\n",
      "> WORLD 18.25\n",
      ">world 17.73\n",
      "> worlds 17.61\n",
      ">World 17.05\n",
      "> mathemat 17.02\n",
      "> globe 16.99\n",
      "> millenn 16.83\n",
      "> challeng 16.62\n",
      "> tremend 16.62\n",
      "> universe 16.43\n",
      "> conflic 16.32\n",
      "> World 16.22\n",
      "> enthusi 16.12\n",
      "> neighb 16.11\n",
      ">senal 16.06\n",
      "> encount 15.98\n",
      "> conclud 15.97\n",
      "> unden 15.94\n",
      "> livest 15.93\n"
     ]
    }
   ],
   "source": [
    "ranked = word_embedding_kernel.argsort(descending=True)\n",
    "print(\"::: Words :::\")\n",
    "for i in range(20):\n",
    "    print(f\">{gpt2_tokenizer.decode(ranked[i])} {word_embedding_kernel[ranked[i]]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02183ad4-bcf9-494b-a01b-32b1cb77d178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "Top 10 ranks\n",
      "[2947, 486, 800, 450, 3065, 1948, 802, 678, 1401, 933]\n",
      "Logit offset per unit of activation\n",
      "[4.5515857 3.5348296 3.462263  3.4285254 3.2951808 3.225283  3.127183\n",
      " 3.115718  3.099147  2.89163  ]\n",
      "Layer 1\n",
      "Top 10 ranks\n",
      "[8, 1675, 2814, 1652, 2450, 107, 2720, 2309, 1781, 2440]\n",
      "Logit offset per unit of activation\n",
      "[8.459323  4.3932323 3.3828032 3.2918189 3.233506  3.1199784 2.947613\n",
      " 2.9233322 2.853726  2.8421643]\n",
      "Layer 2\n",
      "Top 10 ranks\n",
      "[2631, 2051, 2027, 1651, 2198, 3037, 1681, 1328, 2315, 1611]\n",
      "Logit offset per unit of activation\n",
      "[6.841717  5.04234   4.665663  4.40021   3.9743469 3.8107488 3.7076836\n",
      " 3.5124412 3.4829688 3.0682342]\n",
      "Layer 3\n",
      "Top 10 ranks\n",
      "[1616, 1792, 1235, 3054, 1921, 2413, 748, 489, 509, 2547]\n",
      "Logit offset per unit of activation\n",
      "[3.9193954 3.8351953 3.5292473 3.4618466 3.4280593 3.3627706 3.2452142\n",
      " 3.1960106 3.1856818 3.073351 ]\n",
      "Layer 4\n",
      "Top 10 ranks\n",
      "[1590, 1100, 2381, 1404, 2982, 2963, 1862, 1105, 1594, 513]\n",
      "Logit offset per unit of activation\n",
      "[5.469589  4.6596394 3.934458  3.8008077 3.558345  3.4939382 3.4763083\n",
      " 3.232699  3.0624704 3.0588424]\n",
      "Layer 5\n",
      "Top 10 ranks\n",
      "[234, 1849, 541, 2476, 2478, 699, 311, 1235, 1707, 2552]\n",
      "Logit offset per unit of activation\n",
      "[4.054873  4.0506124 3.935595  3.8489375 3.712501  3.482507  3.4487636\n",
      " 3.2729316 3.2486835 3.233331 ]\n",
      "Layer 6\n",
      "Top 10 ranks\n",
      "[31, 2409, 932, 591, 1609, 1501, 1288, 1118, 150, 2703]\n",
      "Logit offset per unit of activation\n",
      "[5.414434  5.205103  5.145586  4.4144373 4.08576   4.0151258 3.9363816\n",
      " 3.8383358 3.7302227 3.7198749]\n",
      "Layer 7\n",
      "Top 10 ranks\n",
      "[23, 1024, 2409, 2176, 1147, 2643, 1346, 1645, 2619, 1425]\n",
      "Logit offset per unit of activation\n",
      "[9.054527  6.4602313 6.03426   5.765163  5.621607  5.0685444 4.9493437\n",
      " 4.9449525 4.6738234 4.6403294]\n",
      "Layer 8\n",
      "Top 10 ranks\n",
      "[1604, 2120, 105, 1304, 2112, 1141, 2469, 2169, 660, 2630]\n",
      "Logit offset per unit of activation\n",
      "[14.127003  10.91819    9.01502    7.26036    6.4237957  6.119693\n",
      "  5.746883   5.7089014  5.6834974  4.954172 ]\n",
      "Layer 9\n",
      "Top 10 ranks\n",
      "[2736, 2238, 2711, 1236, 1182, 546, 2854, 242, 1825, 741]\n",
      "Logit offset per unit of activation\n",
      "[12.02658   11.008651  10.978638  10.172764   9.726135   8.33195\n",
      "  8.078387   6.212037   6.0317535  5.956967 ]\n",
      "Layer 10\n",
      "Top 10 ranks\n",
      "[823, 609, 608, 1359, 2081, 257, 148, 2029, 1276, 2020]\n",
      "Logit offset per unit of activation\n",
      "[28.761051 25.721731 19.694658 19.268373 18.528105 16.916576 16.719763\n",
      " 15.525524 14.4207   13.866212]\n",
      "Layer 11\n",
      "Top 10 ranks\n",
      "[541, 2604, 459, 2227, 553, 2256, 2013, 3033, 2484, 1480]\n",
      "Logit offset per unit of activation\n",
      "[48.160336 35.68578  35.63632  34.842056 33.19827  32.771507 22.043036\n",
      " 21.84559  21.736944 21.588835]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's see if we can figure out the most salient MLP activations\n",
    "for i in range(len(hidden_kernels)):\n",
    "    top_10 = hidden_kernels[i].argsort(descending=True)[:10]\n",
    "    print(\"Layer\", i)\n",
    "    print(\"Top 10 ranks\")\n",
    "    print(top_10.tolist())\n",
    "    print(\"Logit offset per unit of activation\")\n",
    "    print(hidden_kernels[i][top_10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a8c61-fdde-4909-9836-cc55a64dc71e",
   "metadata": {},
   "source": [
    "## Peeling back the layers\n",
    "\n",
    "It seems like layer 11 (the last layer) is, in general, the strongest contributor to the token \" world\", with neuron 541 providing +48.16 activation per unit of activation. (I guess we do still need to consider LayerNorm! But the prior activations are significantly weaker still.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f593ee4-33bf-490c-9205-f6d299441ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_qkv_and_mlp_weights(gpt2, layer_num):\n",
    "    qkv_weight = gpt2.transformer.h[layer_num].attn.c_attn.weight\n",
    "    # torch.split returns \"views\", which are more memory-efficient than creating slices I think\n",
    "    query_weights, key_weights, value_weights = torch.split(qkv_weight, split_size, dim=-1)\n",
    "    \n",
    "    query_weights_per_head = torch.split(query_weights, head_dim, dim=-1)\n",
    "    key_weights_per_head = torch.split(key_weights, head_dim, dim=-1)\n",
    "    value_weights_per_head = torch.split(key_weights, head_dim, dim=-1)\n",
    "    \n",
    "    W_mlp_per_head = torch.split(gpt2.transformer.h[layer_num].mlp.c_fc.weight, head_dim, dim=0)\n",
    "    \n",
    "    return (query_weights_per_head, key_weights_per_head, value_weights_per_head, W_mlp_per_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "809edfef-6608-4257-a128-86009fc73d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_kernel(kernel):\n",
    "    word_embedding_kernel, pos_embedding_kernel, hidden_kernels = kernel\n",
    "    ranked = word_embedding_kernel.argsort(descending=True)\n",
    "#     print(\"::: What word embeddings trigger this? :::\")\n",
    "#     for i in range(25):\n",
    "#         print(f\">{gpt2_tokenizer.decode(ranked[i])} {word_embedding_kernel[ranked[i]]:.2f}\")\n",
    "        \n",
    "#     print(\"::: What Positional embeddings trigger this? :::\")\n",
    "#     ranked = pos_embedding_kernel.argsort(descending=True)\n",
    "#     for i in range(10):\n",
    "#         print(f\"pos {ranked[i]}: {pos_embedding_kernel[ranked[i]]:.2f}\")\n",
    "\n",
    "    # Now, let's see if we can figure out the most salient MLP activations\n",
    "    print(\"::: What prior MLP neurons trigger this? :::\")\n",
    "    k = 1\n",
    "    for i in range(len(hidden_kernels)):\n",
    "        top_k = hidden_kernels[i].argsort(descending=True)[:k]\n",
    "        print(\"Layer\", i)\n",
    "        print(f\"Top {k} ranks\")\n",
    "        print(top_k.tolist())\n",
    "        print(\"Logit offset per unit of activation\")\n",
    "        print(hidden_kernels[i][top_k].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb93c5e7-be15-412d-993d-d4244b25c564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[2496]\n",
      "Logit offset per unit of activation\n",
      "[0.5958279]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[962]\n",
      "Logit offset per unit of activation\n",
      "[0.2955079]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[2942]\n",
      "Logit offset per unit of activation\n",
      "[0.22707301]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[41]\n",
      "Logit offset per unit of activation\n",
      "[0.21945603]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2253]\n",
      "Logit offset per unit of activation\n",
      "[0.3362456]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1206]\n",
      "Logit offset per unit of activation\n",
      "[0.20363009]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[1824]\n",
      "Logit offset per unit of activation\n",
      "[0.23731661]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[1540]\n",
      "Logit offset per unit of activation\n",
      "[0.28071463]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[2560]\n",
      "Logit offset per unit of activation\n",
      "[0.27562135]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[3026]\n",
      "Logit offset per unit of activation\n",
      "[0.29333735]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1276]\n",
      "Logit offset per unit of activation\n",
      "[0.5318454]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2728]\n",
      "Logit offset per unit of activation\n",
      "[0.4697923]\n",
      "HEAD 1\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1726]\n",
      "Logit offset per unit of activation\n",
      "[0.37679923]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[962]\n",
      "Logit offset per unit of activation\n",
      "[0.29531887]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[2815]\n",
      "Logit offset per unit of activation\n",
      "[0.2776808]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2108]\n",
      "Logit offset per unit of activation\n",
      "[0.2991896]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2645]\n",
      "Logit offset per unit of activation\n",
      "[0.25032753]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[2895]\n",
      "Logit offset per unit of activation\n",
      "[0.30084428]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[2538]\n",
      "Logit offset per unit of activation\n",
      "[0.3933754]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[1509]\n",
      "Logit offset per unit of activation\n",
      "[0.28987437]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[532]\n",
      "Logit offset per unit of activation\n",
      "[0.35116452]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[2414]\n",
      "Logit offset per unit of activation\n",
      "[0.38665342]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[210]\n",
      "Logit offset per unit of activation\n",
      "[0.4430177]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[789]\n",
      "Logit offset per unit of activation\n",
      "[0.5407893]\n",
      "HEAD 2\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[416]\n",
      "Logit offset per unit of activation\n",
      "[0.66603863]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[2355]\n",
      "Logit offset per unit of activation\n",
      "[0.7324564]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[2360]\n",
      "Logit offset per unit of activation\n",
      "[0.40930933]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[30]\n",
      "Logit offset per unit of activation\n",
      "[0.3157466]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2380]\n",
      "Logit offset per unit of activation\n",
      "[0.29368567]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[2903]\n",
      "Logit offset per unit of activation\n",
      "[0.32116976]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[884]\n",
      "Logit offset per unit of activation\n",
      "[0.50282395]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[578]\n",
      "Logit offset per unit of activation\n",
      "[0.36144772]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[129]\n",
      "Logit offset per unit of activation\n",
      "[0.45783427]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1503]\n",
      "Logit offset per unit of activation\n",
      "[0.512172]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1480]\n",
      "Logit offset per unit of activation\n",
      "[0.45223123]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2870]\n",
      "Logit offset per unit of activation\n",
      "[1.0271758]\n",
      "HEAD 3\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1726]\n",
      "Logit offset per unit of activation\n",
      "[0.65090644]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[1745]\n",
      "Logit offset per unit of activation\n",
      "[0.4914043]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1416]\n",
      "Logit offset per unit of activation\n",
      "[0.27571222]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2697]\n",
      "Logit offset per unit of activation\n",
      "[0.30810565]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[988]\n",
      "Logit offset per unit of activation\n",
      "[0.29206678]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1743]\n",
      "Logit offset per unit of activation\n",
      "[0.27673888]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[2409]\n",
      "Logit offset per unit of activation\n",
      "[0.28568956]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[360]\n",
      "Logit offset per unit of activation\n",
      "[0.26765275]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[6]\n",
      "Logit offset per unit of activation\n",
      "[0.36482957]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1303]\n",
      "Logit offset per unit of activation\n",
      "[0.44166213]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[2976]\n",
      "Logit offset per unit of activation\n",
      "[0.46003914]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[208]\n",
      "Logit offset per unit of activation\n",
      "[0.51566595]\n",
      "HEAD 4\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1278]\n",
      "Logit offset per unit of activation\n",
      "[0.76856065]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[7]\n",
      "Logit offset per unit of activation\n",
      "[0.46887124]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[989]\n",
      "Logit offset per unit of activation\n",
      "[0.32463324]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2399]\n",
      "Logit offset per unit of activation\n",
      "[0.29398826]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2619]\n",
      "Logit offset per unit of activation\n",
      "[0.42847484]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1924]\n",
      "Logit offset per unit of activation\n",
      "[0.28438324]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[31]\n",
      "Logit offset per unit of activation\n",
      "[0.38949174]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2953]\n",
      "Logit offset per unit of activation\n",
      "[0.23558025]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[2664]\n",
      "Logit offset per unit of activation\n",
      "[0.40061346]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1719]\n",
      "Logit offset per unit of activation\n",
      "[0.31050062]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[789]\n",
      "Logit offset per unit of activation\n",
      "[0.31499112]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2123]\n",
      "Logit offset per unit of activation\n",
      "[1.1992592]\n",
      "HEAD 5\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[2454]\n",
      "Logit offset per unit of activation\n",
      "[0.6583697]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[1086]\n",
      "Logit offset per unit of activation\n",
      "[0.4913819]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1584]\n",
      "Logit offset per unit of activation\n",
      "[0.4152131]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2053]\n",
      "Logit offset per unit of activation\n",
      "[0.42418066]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2709]\n",
      "Logit offset per unit of activation\n",
      "[0.4924221]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[2084]\n",
      "Logit offset per unit of activation\n",
      "[0.4725395]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[127]\n",
      "Logit offset per unit of activation\n",
      "[0.48545137]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2550]\n",
      "Logit offset per unit of activation\n",
      "[0.6210538]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[1780]\n",
      "Logit offset per unit of activation\n",
      "[0.5321551]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1152]\n",
      "Logit offset per unit of activation\n",
      "[0.4280585]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1735]\n",
      "Logit offset per unit of activation\n",
      "[0.48850566]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2123]\n",
      "Logit offset per unit of activation\n",
      "[1.4621538]\n",
      "HEAD 6\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1817]\n",
      "Logit offset per unit of activation\n",
      "[1.3286074]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[146]\n",
      "Logit offset per unit of activation\n",
      "[0.79410225]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[3034]\n",
      "Logit offset per unit of activation\n",
      "[0.5404352]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[41]\n",
      "Logit offset per unit of activation\n",
      "[0.60628587]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2709]\n",
      "Logit offset per unit of activation\n",
      "[1.1026834]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[2256]\n",
      "Logit offset per unit of activation\n",
      "[0.45911917]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[2987]\n",
      "Logit offset per unit of activation\n",
      "[0.36714268]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2651]\n",
      "Logit offset per unit of activation\n",
      "[0.4449588]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[1307]\n",
      "Logit offset per unit of activation\n",
      "[0.4315616]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[2383]\n",
      "Logit offset per unit of activation\n",
      "[0.3834027]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1298]\n",
      "Logit offset per unit of activation\n",
      "[0.7428735]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[1025]\n",
      "Logit offset per unit of activation\n",
      "[0.6585708]\n",
      "HEAD 7\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1641]\n",
      "Logit offset per unit of activation\n",
      "[0.7341529]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[242]\n",
      "Logit offset per unit of activation\n",
      "[0.5050046]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1269]\n",
      "Logit offset per unit of activation\n",
      "[0.41800952]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[1751]\n",
      "Logit offset per unit of activation\n",
      "[0.32238376]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2709]\n",
      "Logit offset per unit of activation\n",
      "[0.55232894]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1752]\n",
      "Logit offset per unit of activation\n",
      "[0.33534804]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[1653]\n",
      "Logit offset per unit of activation\n",
      "[0.38810077]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2651]\n",
      "Logit offset per unit of activation\n",
      "[0.40592632]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[1549]\n",
      "Logit offset per unit of activation\n",
      "[0.46757305]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[2788]\n",
      "Logit offset per unit of activation\n",
      "[0.38273802]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[955]\n",
      "Logit offset per unit of activation\n",
      "[0.42139784]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2462]\n",
      "Logit offset per unit of activation\n",
      "[0.50985897]\n",
      "HEAD 8\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1594]\n",
      "Logit offset per unit of activation\n",
      "[1.126454]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[172]\n",
      "Logit offset per unit of activation\n",
      "[0.5668936]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1597]\n",
      "Logit offset per unit of activation\n",
      "[0.86827636]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2108]\n",
      "Logit offset per unit of activation\n",
      "[0.9166168]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2253]\n",
      "Logit offset per unit of activation\n",
      "[0.5637596]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[303]\n",
      "Logit offset per unit of activation\n",
      "[0.34846887]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[1533]\n",
      "Logit offset per unit of activation\n",
      "[0.371252]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[1148]\n",
      "Logit offset per unit of activation\n",
      "[0.36229026]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[415]\n",
      "Logit offset per unit of activation\n",
      "[0.35560146]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1090]\n",
      "Logit offset per unit of activation\n",
      "[0.45983425]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[609]\n",
      "Logit offset per unit of activation\n",
      "[0.52869433]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2870]\n",
      "Logit offset per unit of activation\n",
      "[2.017693]\n",
      "HEAD 9\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[2940]\n",
      "Logit offset per unit of activation\n",
      "[0.58907044]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[2396]\n",
      "Logit offset per unit of activation\n",
      "[0.56036264]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1269]\n",
      "Logit offset per unit of activation\n",
      "[0.5519605]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[2108]\n",
      "Logit offset per unit of activation\n",
      "[0.33069897]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2963]\n",
      "Logit offset per unit of activation\n",
      "[0.47351623]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1752]\n",
      "Logit offset per unit of activation\n",
      "[0.40946802]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[684]\n",
      "Logit offset per unit of activation\n",
      "[0.6599988]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[3066]\n",
      "Logit offset per unit of activation\n",
      "[0.5215999]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[1549]\n",
      "Logit offset per unit of activation\n",
      "[0.45385957]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[2886]\n",
      "Logit offset per unit of activation\n",
      "[0.41312394]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1813]\n",
      "Logit offset per unit of activation\n",
      "[0.5271319]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2462]\n",
      "Logit offset per unit of activation\n",
      "[0.86079526]\n",
      "HEAD 10\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1726]\n",
      "Logit offset per unit of activation\n",
      "[1.434891]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[2355]\n",
      "Logit offset per unit of activation\n",
      "[0.6573507]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[1269]\n",
      "Logit offset per unit of activation\n",
      "[0.49283567]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[1612]\n",
      "Logit offset per unit of activation\n",
      "[0.5916905]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[919]\n",
      "Logit offset per unit of activation\n",
      "[0.44971815]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[1866]\n",
      "Logit offset per unit of activation\n",
      "[0.5012671]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[1653]\n",
      "Logit offset per unit of activation\n",
      "[0.49182844]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2156]\n",
      "Logit offset per unit of activation\n",
      "[0.3946875]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[2968]\n",
      "Logit offset per unit of activation\n",
      "[0.52290267]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1506]\n",
      "Logit offset per unit of activation\n",
      "[0.35323006]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[1340]\n",
      "Logit offset per unit of activation\n",
      "[0.5650614]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2870]\n",
      "Logit offset per unit of activation\n",
      "[1.228836]\n",
      "HEAD 11\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "Layer 0\n",
      "Top 1 ranks\n",
      "[1594]\n",
      "Logit offset per unit of activation\n",
      "[0.9015443]\n",
      "Layer 1\n",
      "Top 1 ranks\n",
      "[242]\n",
      "Logit offset per unit of activation\n",
      "[0.7646102]\n",
      "Layer 2\n",
      "Top 1 ranks\n",
      "[3034]\n",
      "Logit offset per unit of activation\n",
      "[0.5962418]\n",
      "Layer 3\n",
      "Top 1 ranks\n",
      "[1751]\n",
      "Logit offset per unit of activation\n",
      "[0.47640505]\n",
      "Layer 4\n",
      "Top 1 ranks\n",
      "[2709]\n",
      "Logit offset per unit of activation\n",
      "[0.45342577]\n",
      "Layer 5\n",
      "Top 1 ranks\n",
      "[886]\n",
      "Logit offset per unit of activation\n",
      "[0.25213864]\n",
      "Layer 6\n",
      "Top 1 ranks\n",
      "[2622]\n",
      "Logit offset per unit of activation\n",
      "[0.2745582]\n",
      "Layer 7\n",
      "Top 1 ranks\n",
      "[2651]\n",
      "Logit offset per unit of activation\n",
      "[0.40991434]\n",
      "Layer 8\n",
      "Top 1 ranks\n",
      "[1549]\n",
      "Logit offset per unit of activation\n",
      "[0.47700635]\n",
      "Layer 9\n",
      "Top 1 ranks\n",
      "[1047]\n",
      "Logit offset per unit of activation\n",
      "[0.32531336]\n",
      "Layer 10\n",
      "Top 1 ranks\n",
      "[405]\n",
      "Logit offset per unit of activation\n",
      "[0.34609413]\n",
      "Layer 11\n",
      "Top 1 ranks\n",
      "[2870]\n",
      "Logit offset per unit of activation\n",
      "[1.2023811]\n"
     ]
    }
   ],
   "source": [
    "# this is what the MLP neuron activates for (only tracing for this particular head though)\n",
    "layer = 11\n",
    "neuron = 541\n",
    "query_weights_per_head, key_weights_per_head, value_weights_per_head, W_mlp_per_head = get_qkv_and_mlp_weights(gpt2, layer)\n",
    "\n",
    "for head in range(12):\n",
    "    print(\"HEAD\", head)\n",
    "    residual_stream_MLP_target = value_weights_per_head[head] @ W_mlp_per_head[head][:, neuron]\n",
    "    # residual_stream_MLP_target = gpt2.lm_head.weight[world_tok]\n",
    "    kernel = get_sparse_activation_kernel(gpt2, residual_stream_MLP_target, until_layer=12)\n",
    "    visualize_kernel(kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e89f0-8467-4413-aa4c-02d724c58ca5",
   "metadata": {},
   "source": [
    "### Now let's see if we can figure out what *earlier* neurons contribute to THIS one! It seems that layer 0, neuron 2496 triggers this neuron's activation from the value-vector side. What does *this* neuron look for?\n",
    "\n",
    "Realizing that part of the difficulty of mechanistic interpretability is figuring out what individual MLP neurons are responsible for in an encapsulated way. But maybe a forward-facing approach will work well.\n",
    "\n",
    "`world_tok, hello_tok = 995, 15496`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5070ea61-ae78-49a0-a843-59bf6ab01914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_weights_per_head, key_weights_per_head, value_weights_per_head, W_mlp_per_head = get_qkv_and_mlp_weights(gpt2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bc7e467f-88f1-4d8d-b493-66481070bb75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD 0\n",
      "::: What word embeddings trigger this? :::\n",
      ">agos 0.34\n",
      ">ersion 0.32\n",
      ">IENT 0.32\n",
      ">eps 0.32\n",
      ">Fit 0.30\n",
      ">arial 0.30\n",
      ">portion 0.30\n",
      ">arella 0.30\n",
      ">illard 0.30\n",
      ">ć 0.30\n",
      ">ITH 0.29\n",
      ">igree 0.29\n",
      ">ppo 0.28\n",
      ">FM 0.28\n",
      "> OECD 0.28\n",
      "> AHL 0.28\n",
      ">ée 0.28\n",
      "> posterior 0.28\n",
      "> Pixel 0.28\n",
      ">rection 0.27\n",
      ">inar 0.27\n",
      "> appealed 0.27\n",
      ">ersed 0.27\n",
      ">ient 0.27\n",
      "> Tiff 0.27\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 624: 0.10\n",
      "pos 604: 0.10\n",
      "pos 637: 0.10\n",
      "pos 611: 0.10\n",
      "pos 625: 0.10\n",
      "pos 631: 0.10\n",
      "pos 635: 0.10\n",
      "pos 626: 0.10\n",
      "pos 629: 0.10\n",
      "pos 628: 0.10\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 1\n",
      "::: What word embeddings trigger this? :::\n",
      ">zynski 0.36\n",
      ">henko 0.34\n",
      "> Sr 0.32\n",
      "> reportedly 0.32\n",
      "> Frem 0.32\n",
      ">worth 0.32\n",
      ">bourg 0.31\n",
      ">enegger 0.31\n",
      "> Diaz 0.30\n",
      "> namesake 0.30\n",
      ">zinski 0.29\n",
      ">zech 0.29\n",
      ">coni 0.28\n",
      "> Annotations 0.28\n",
      "> classmate 0.28\n",
      "> prestigious 0.27\n",
      "> esteemed 0.27\n",
      "> agreeing 0.27\n",
      "> illustrious 0.27\n",
      "> Lans 0.27\n",
      ">arov 0.27\n",
      ">andestine 0.27\n",
      "> ATK 0.27\n",
      "> agreed 0.26\n",
      "> birthday 0.26\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 0: 0.27\n",
      "pos 329: 0.17\n",
      "pos 331: 0.17\n",
      "pos 323: 0.17\n",
      "pos 344: 0.17\n",
      "pos 327: 0.17\n",
      "pos 324: 0.17\n",
      "pos 340: 0.17\n",
      "pos 333: 0.17\n",
      "pos 330: 0.17\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 2\n",
      "::: What word embeddings trigger this? :::\n",
      ">heid 0.40\n",
      ">abwe 0.40\n",
      ">innie 0.40\n",
      ">ijk 0.37\n",
      ">ahu 0.37\n",
      ">ofi 0.36\n",
      ">porting 0.36\n",
      ">amba 0.35\n",
      ">sem 0.34\n",
      "> und 0.32\n",
      ">henko 0.32\n",
      ">aito 0.32\n",
      ">zin 0.32\n",
      ">akis 0.32\n",
      ">ingo 0.31\n",
      ">andals 0.31\n",
      ">rov 0.31\n",
      ">zi 0.30\n",
      ">afa 0.30\n",
      "> kil 0.30\n",
      ">oros 0.30\n",
      ">aults 0.30\n",
      ">soever 0.30\n",
      ">erry 0.29\n",
      ">gom 0.29\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 1: 0.26\n",
      "pos 2: 0.22\n",
      "pos 0: 0.20\n",
      "pos 665: 0.19\n",
      "pos 662: 0.19\n",
      "pos 664: 0.19\n",
      "pos 655: 0.19\n",
      "pos 656: 0.19\n",
      "pos 651: 0.19\n",
      "pos 653: 0.19\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 3\n",
      "::: What word embeddings trigger this? :::\n",
      ">isSpecialOrderable 0.71\n",
      ">DragonMagazine 0.71\n",
      ">aldo 0.70\n",
      ">覚醒 0.69\n",
      "> Flavoring 0.69\n",
      ">SourceFile 0.69\n",
      ">uzzle 0.67\n",
      ">ORPG 0.67\n",
      "> glim 0.66\n",
      ">quickShipAvailable 0.66\n",
      "> Reef 0.65\n",
      ">�� 0.65\n",
      ">Minecraft 0.65\n",
      ">�� 0.65\n",
      ">abwe 0.63\n",
      ">abiding 0.63\n",
      ">adden 0.62\n",
      ">educ 0.62\n",
      "> viz 0.62\n",
      ">ahime 0.62\n",
      ">izoph 0.61\n",
      ">ワン 0.61\n",
      ">�� 0.61\n",
      ">sama 0.60\n",
      ">landish 0.60\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 1012: 0.02\n",
      "pos 1005: 0.02\n",
      "pos 1014: 0.02\n",
      "pos 14: 0.02\n",
      "pos 13: 0.02\n",
      "pos 1015: 0.02\n",
      "pos 1010: 0.02\n",
      "pos 17: 0.02\n",
      "pos 18: 0.02\n",
      "pos 999: 0.02\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 4\n",
      "::: What word embeddings trigger this? :::\n",
      "> Caption 0.43\n",
      ">ecause 0.42\n",
      ">gency 0.41\n",
      ">erville 0.41\n",
      ">MSN 0.41\n",
      ">ankind 0.38\n",
      "> Haram 0.36\n",
      ">appiness 0.35\n",
      "> WARN 0.35\n",
      "> Now 0.34\n",
      ">livion 0.34\n",
      ">English 0.34\n",
      ">vity 0.34\n",
      ">uties 0.34\n",
      "> murd 0.33\n",
      ">friends 0.33\n",
      ">V 0.33\n",
      ">adders 0.33\n",
      ">essage 0.33\n",
      "> satisfaction 0.33\n",
      "> Activities 0.32\n",
      ">earances 0.32\n",
      ">oku 0.32\n",
      "> reward 0.32\n",
      "> voice 0.32\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 7: 0.24\n",
      "pos 6: 0.24\n",
      "pos 8: 0.24\n",
      "pos 5: 0.24\n",
      "pos 9: 0.24\n",
      "pos 10: 0.23\n",
      "pos 4: 0.23\n",
      "pos 12: 0.23\n",
      "pos 14: 0.23\n",
      "pos 13: 0.22\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 5\n",
      "::: What word embeddings trigger this? :::\n",
      ">sic 0.34\n",
      ">rored 0.33\n",
      ">burst 0.32\n",
      "> wearer 0.32\n",
      ">verson 0.31\n",
      "> stitches 0.30\n",
      ">clud 0.30\n",
      ">uner 0.30\n",
      "> SetFontSize 0.30\n",
      ">stretched 0.30\n",
      "> offending 0.29\n",
      "> blockers 0.29\n",
      "> haircut 0.29\n",
      ">plet 0.28\n",
      ">worm 0.28\n",
      ">pperc 0.28\n",
      "> aggress 0.27\n",
      ">acho 0.27\n",
      ">nikov 0.27\n",
      ">assed 0.27\n",
      "> viol 0.27\n",
      ">multipl 0.27\n",
      ">rator 0.27\n",
      ">antz 0.26\n",
      ">oufl 0.26\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 596: 0.08\n",
      "pos 598: 0.08\n",
      "pos 602: 0.08\n",
      "pos 623: 0.08\n",
      "pos 609: 0.08\n",
      "pos 594: 0.08\n",
      "pos 614: 0.08\n",
      "pos 591: 0.08\n",
      "pos 592: 0.08\n",
      "pos 603: 0.08\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 6\n",
      "::: What word embeddings trigger this? :::\n",
      "> Shutterstock 0.61\n",
      "> Deadline 0.59\n",
      "> EDIT 0.53\n",
      ">avascript 0.52\n",
      ">CLAIM 0.51\n",
      "> Accessed 0.51\n",
      ">report 0.51\n",
      ">Report 0.50\n",
      "> Attribution 0.50\n",
      ">sheet 0.49\n",
      ">Wiki 0.49\n",
      "> Bungie 0.49\n",
      "> Pastebin 0.49\n",
      "> Wiki 0.49\n",
      "> publication 0.49\n",
      "> Disclosure 0.48\n",
      "> HIT 0.48\n",
      ">Site 0.48\n",
      "> VERS 0.48\n",
      "><|endoftext|> 0.47\n",
      "> BBC 0.47\n",
      "> deadline 0.47\n",
      "> Ubisoft 0.46\n",
      ">nesday 0.46\n",
      ">notes 0.46\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 474: 0.25\n",
      "pos 462: 0.25\n",
      "pos 460: 0.25\n",
      "pos 472: 0.25\n",
      "pos 463: 0.25\n",
      "pos 470: 0.25\n",
      "pos 482: 0.25\n",
      "pos 476: 0.25\n",
      "pos 469: 0.25\n",
      "pos 480: 0.25\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 7\n",
      "::: What word embeddings trigger this? :::\n",
      "> Samson 0.37\n",
      "> Mour 0.29\n",
      ">alid 0.29\n",
      ">itimate 0.28\n",
      ">onne 0.26\n",
      ">aroo 0.26\n",
      "> Bak 0.25\n",
      ">amen 0.25\n",
      ">rier 0.24\n",
      ">outh 0.24\n",
      ">ivari 0.24\n",
      "> kidn 0.24\n",
      "> amb 0.24\n",
      "> Temper 0.24\n",
      ">Hig 0.24\n",
      ">hurst 0.24\n",
      "> mainland 0.23\n",
      ">hend 0.23\n",
      ">iba 0.23\n",
      ">apo 0.23\n",
      "> Rib 0.23\n",
      "> revenge 0.23\n",
      ">berg 0.23\n",
      ">adr 0.23\n",
      "> showers 0.22\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 0: 0.35\n",
      "pos 765: 0.16\n",
      "pos 769: 0.16\n",
      "pos 763: 0.16\n",
      "pos 1: 0.16\n",
      "pos 759: 0.15\n",
      "pos 762: 0.15\n",
      "pos 770: 0.15\n",
      "pos 772: 0.15\n",
      "pos 778: 0.15\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 8\n",
      "::: What word embeddings trigger this? :::\n",
      ">, 0.30\n",
      ">- 0.24\n",
      "> and 0.22\n",
      ">. 0.20\n",
      ">ly 0.18\n",
      "> of 0.18\n",
      "> the 0.17\n",
      ">\n",
      " 0.16\n",
      ">/ 0.16\n",
      "> in 0.16\n",
      "> a 0.16\n",
      "> ( 0.15\n",
      "> free 0.15\n",
      "> use 0.14\n",
      "> to 0.14\n",
      ">free 0.14\n",
      ">; 0.13\n",
      "> it 0.13\n",
      "> B 0.13\n",
      "> for 0.13\n",
      "> can 0.12\n",
      "> or 0.12\n",
      ">c 0.12\n",
      ">a 0.12\n",
      "> are 0.12\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 1020: 0.16\n",
      "pos 1021: 0.16\n",
      "pos 1019: 0.15\n",
      "pos 1018: 0.15\n",
      "pos 1022: 0.15\n",
      "pos 1017: 0.14\n",
      "pos 1016: 0.13\n",
      "pos 1015: 0.13\n",
      "pos 1013: 0.12\n",
      "pos 1012: 0.12\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 9\n",
      "::: What word embeddings trigger this? :::\n",
      ">ements 0.41\n",
      ">ilitating 0.41\n",
      ">holder 0.41\n",
      ">rogram 0.41\n",
      ">uitous 0.41\n",
      ">Upload 0.40\n",
      ">ording 0.40\n",
      ">heses 0.38\n",
      ">itures 0.38\n",
      ">inement 0.38\n",
      ">ulence 0.38\n",
      ">lasses 0.38\n",
      ">ework 0.38\n",
      ">holders 0.37\n",
      "> cutter 0.37\n",
      ">ruption 0.37\n",
      "> Swordsman 0.37\n",
      ">uberty 0.37\n",
      "> tamp 0.36\n",
      "> blender 0.36\n",
      ">ilities 0.36\n",
      ">atively 0.36\n",
      ">pieces 0.36\n",
      "> applic 0.35\n",
      ">htaking 0.35\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 7: 0.22\n",
      "pos 8: 0.22\n",
      "pos 6: 0.22\n",
      "pos 9: 0.22\n",
      "pos 10: 0.21\n",
      "pos 5: 0.21\n",
      "pos 11: 0.21\n",
      "pos 12: 0.20\n",
      "pos 13: 0.20\n",
      "pos 14: 0.19\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 10\n",
      "::: What word embeddings trigger this? :::\n",
      "> precious 0.24\n",
      "> account 0.22\n",
      ">orters 0.21\n",
      "> comfort 0.20\n",
      "> sake 0.20\n",
      ">ORTS 0.20\n",
      "> imagination 0.19\n",
      ">INT 0.19\n",
      "> own 0.18\n",
      "> employers 0.18\n",
      "> ever 0.18\n",
      "> margins 0.18\n",
      "> feelings 0.18\n",
      "> spared 0.18\n",
      "> enjoyment 0.17\n",
      "> tags 0.17\n",
      "> advertisers 0.17\n",
      ">dogs 0.17\n",
      "> bubble 0.17\n",
      "> id 0.16\n",
      "> sterling 0.16\n",
      "> organs 0.16\n",
      "> less 0.16\n",
      "> particular 0.16\n",
      "> words 0.16\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 1015: 0.22\n",
      "pos 1021: 0.21\n",
      "pos 1017: 0.21\n",
      "pos 1018: 0.21\n",
      "pos 1019: 0.21\n",
      "pos 1022: 0.21\n",
      "pos 616: 0.21\n",
      "pos 1020: 0.21\n",
      "pos 602: 0.21\n",
      "pos 1016: 0.21\n",
      "::: What prior MLP neurons trigger this? :::\n",
      "HEAD 11\n",
      "::: What word embeddings trigger this? :::\n",
      ">thing 0.52\n",
      ">etc 0.42\n",
      ">DERR 0.41\n",
      "> Sutton 0.40\n",
      ">MORE 0.40\n",
      ">reddits 0.40\n",
      ">odd 0.39\n",
      ">Redd 0.39\n",
      ">corn 0.39\n",
      ">iott 0.39\n",
      ">NFL 0.39\n",
      ">TIT 0.38\n",
      ">#$#$ 0.38\n",
      ">wed 0.38\n",
      ">deals 0.37\n",
      ">catentry 0.37\n",
      ">STDOUT 0.36\n",
      "> ILCS 0.36\n",
      ">Article 0.36\n",
      ">imeo 0.36\n",
      ">Market 0.36\n",
      "> fixme 0.36\n",
      ">photo 0.35\n",
      ">externalActionCode 0.35\n",
      ">abb 0.35\n",
      "::: What Positional embeddings trigger this? :::\n",
      "pos 288: 0.15\n",
      "pos 292: 0.14\n",
      "pos 296: 0.14\n",
      "pos 284: 0.14\n",
      "pos 286: 0.14\n",
      "pos 275: 0.14\n",
      "pos 285: 0.14\n",
      "pos 291: 0.14\n",
      "pos 306: 0.14\n",
      "pos 298: 0.14\n",
      "::: What prior MLP neurons trigger this? :::\n"
     ]
    }
   ],
   "source": [
    "for head in range(12):\n",
    "    print(\"HEAD\", head)\n",
    "    residual_stream_MLP_target = value_weights_per_head[head] @ W_mlp_per_head[head][:, 2496]\n",
    "    kernel = get_sparse_activation_kernel(gpt2, residual_stream_MLP_target, until_layer=0)\n",
    "    visualize_kernel(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9f5c7-b841-4737-9be5-695877e6e953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse",
   "language": "python",
   "name": "sparse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
